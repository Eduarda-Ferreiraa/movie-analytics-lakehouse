{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80a18374-d075-4839-a98f-9a72c8c7e7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "  .appName(\"DW\") \\\n",
    "  .enableHiveSupport() \\\n",
    "  .config(\n",
    "    \"hive.metastore.uris\",\n",
    "    \"thrift://hive-metastore:9083\"\n",
    "  ) \\\n",
    "  .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b519d9f2-b801-4d9d-94fd-a2df9dba9000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    CREATE DATABASE IF NOT EXISTS golds LOCATION 'hdfs://hdfs-nn:9000/demo/golds/'\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bda594e-0c2c-44a2-b76b-d09c60143a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed2a50f0-cdc6-42d8-a087-b01d8faea383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dim tables\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS golds.dim_title\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS golds.dim_time\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS golds.dim_platform\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS golds.dim_rating_status\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS golds.dim_person\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS golds.dim_category\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS golds.dim_genre\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS golds.dim_title (\n",
    "  id_title INT,\n",
    "  title STRING,\n",
    "  age_rating STRING,\n",
    "  runtime INT,\n",
    "  studio_name STRING,\n",
    "  type STRING\n",
    ")\n",
    "USING PARQUET\n",
    "LOCATION 'hdfs://hdfs-nn:9000/demo/golds/dim_title'\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS golds.dim_time (\n",
    "  id_time INT,\n",
    "  date DATE,\n",
    "  day INT,\n",
    "  month INT,\n",
    "  year INT,\n",
    "  day_name STRING,\n",
    "  month_name STRING,\n",
    "  is_weekend BOOLEAN,\n",
    "  season STRING\n",
    ")\n",
    "USING PARQUET\n",
    "LOCATION 'hdfs://hdfs-nn:9000/demo/golds/dim_time'\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS golds.dim_platform (\n",
    "  id_platform INT,\n",
    "  platform_name STRING\n",
    ")\n",
    "USING PARQUET\n",
    "LOCATION 'hdfs://hdfs-nn:9000/demo/golds/dim_platform'\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS golds.dim_rating_status (\n",
    "  id_rating_source_status INT,\n",
    "  tomatometer_status STRING,\n",
    "  source STRING\n",
    ")\n",
    "USING PARQUET\n",
    "LOCATION 'hdfs://hdfs-nn:9000/demo/golds/dim_rating_status'\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS golds.dim_person (\n",
    "  id_person INT,\n",
    "  name STRING,\n",
    "  person_type STRING\n",
    ")\n",
    "USING PARQUET\n",
    "LOCATION 'hdfs://hdfs-nn:9000/demo/golds/dim_person'\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS golds.dim_category (\n",
    "  id_award INT,\n",
    "  canonical_category STRING\n",
    ")\n",
    "USING PARQUET\n",
    "LOCATION 'hdfs://hdfs-nn:9000/demo/golds/dim_category'\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS golds.dim_genre (\n",
    "  id_genre INT,\n",
    "  genre_name STRING\n",
    ")\n",
    "USING PARQUET\n",
    "LOCATION 'hdfs://hdfs-nn:9000/demo/golds/dim_genre'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c68a1a13-7c77-4be3-bd4f-0f34c99926f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bridge tables\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS golds.bridge_launch_genre\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS golds.bridge_views_platform\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS golds.bridge_launch_genre (\n",
    "  id_genre INT,\n",
    "  id_group_genre INT\n",
    ")\n",
    "USING PARQUET\n",
    "LOCATION 'hdfs://hdfs-nn:9000/demo/golds/bridge_title_genre'\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS golds.bridge_views_platform (\n",
    "  id_platform INT,\n",
    "  id_group_platform INT\n",
    ")\n",
    "USING PARQUET\n",
    "LOCATION 'hdfs://hdfs-nn:9000/demo/golds/bridge_views_platform'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afcab357-3ae1-4d9a-aa56-0d22c3030577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fact tables\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS golds.views\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS golds.rating\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS golds.participation\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS golds.awards\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS golds.launch\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS golds.views (\n",
    "  id_title INT,\n",
    "  id_time INT,\n",
    "  id_group_platform INT,\n",
    "  views_price DOUBLE,\n",
    "  views_date INT,\n",
    "  downloads_price DOUBLE,\n",
    "  first_tag_date INT,\n",
    "  days_since_release INT\n",
    ")\n",
    "USING PARQUET\n",
    "LOCATION 'hdfs://hdfs-nn:9000/demo/golds/views'\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS golds.rating (\n",
    "  id_title INT,\n",
    "  id_rating_source_status INT,\n",
    "  id_time INT,\n",
    "  popularity_index STRING,\n",
    "  critic_score DOUBLE,\n",
    "  audience_score DOUBLE,\n",
    "  critic_reviews INT,\n",
    "  audience_reviews INT,\n",
    "  rt_critic_audience STRING\n",
    ")\n",
    "USING PARQUET\n",
    "LOCATION 'hdfs://hdfs-nn:9000/demo/golds/rating'\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS golds.participation (\n",
    "  id_person INT,\n",
    "  id_title INT,\n",
    "  id_time INT\n",
    ")\n",
    "USING PARQUET\n",
    "LOCATION 'hdfs://hdfs-nn:9000/demo/golds/participation'\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS golds.awards (\n",
    "  id_award INT,\n",
    "  id_title INT,\n",
    "  id_time INT,\n",
    "  id_person INT,\n",
    "  is_nomination BOOLEAN,\n",
    "  is_winner BOOLEAN,\n",
    "  is_oscar_nominee BOOLEAN,\n",
    "  is_oscar_winner BOOLEAN\n",
    ")\n",
    "USING PARQUET\n",
    "LOCATION 'hdfs://hdfs-nn:9000/demo/golds/awards';\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS golds.launch (\n",
    "  id_title INT,\n",
    "  id_time INT,\n",
    "  id_group_genre INT,\n",
    "  flag_theaters BOOLEAN,\n",
    "  flag_streaming BOOLEAN,\n",
    "  gross DOUBLE,\n",
    "  budget DOUBLE\n",
    ")\n",
    "USING PARQUET\n",
    "LOCATION 'hdfs://hdfs-nn:9000/demo/golds/launch'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "accc388e-9e47-465f-8724-55b9d087b4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "actorfilms = spark.read.parquet(\"hdfs://hdfs-nn:9000/demo/silver/actorfilms\")\n",
    "audience_reviews = spark.read.parquet(\"hdfs://hdfs-nn:9000/demo/silver/audience_reviews\")\n",
    "boxoffice_info = spark.read.parquet(\"hdfs://hdfs-nn:9000/demo/silver/boxoffice_info\")\n",
    "credits = spark.read.parquet(\"hdfs://hdfs-nn:9000/demo/silver/credits\")\n",
    "critic_reviews = spark.read.parquet(\"hdfs://hdfs-nn:9000/demo/silver/critic_reviews\")\n",
    "dataset_piracy = spark.read.parquet(\"hdfs://hdfs-nn:9000/demo/silver/dataset_piracy\")\n",
    "oscar_fulldata = spark.read.parquet(\"hdfs://hdfs-nn:9000/demo/silver/oscar_fulldata\")\n",
    "rotten_tomatoes_movies = spark.read.parquet(\"hdfs://hdfs-nn:9000/demo/silver/rotten_tomatoes_movies\")\n",
    "titles = spark.read.parquet(\"hdfs://hdfs-nn:9000/demo/silver/titles\")\n",
    "tv_show_links = spark.read.parquet(\"hdfs://hdfs-nn:9000/demo/silver/tv_show_links\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7098ef16-301c-4ad6-9a35-572d6a891992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_time: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- day_name: string (nullable = true)\n",
      " |-- month_name: string (nullable = true)\n",
      " |-- is_weekend: boolean (nullable = true)\n",
      " |-- season: string (nullable = true)\n",
      "\n",
      "+--------+----------+----+-----+----+---------+----------+----------+------+\n",
      "|id_time |date      |day |month|year|day_name |month_name|is_weekend|season|\n",
      "+--------+----------+----+-----+----+---------+----------+----------+------+\n",
      "|19100000|null      |null|null |1910|null     |null      |null      |null  |\n",
      "|19110000|null      |null|null |1911|null     |null      |null      |null  |\n",
      "|19120000|null      |null|null |1912|null     |null      |null      |null  |\n",
      "|19130000|null      |null|null |1913|null     |null      |null      |null  |\n",
      "|19140000|null      |null|null |1914|null     |null      |null      |null  |\n",
      "|19140601|1914-06-01|1   |6    |1914|Monday   |June      |false     |Summer|\n",
      "|19150000|null      |null|null |1915|null     |null      |null      |null  |\n",
      "|19150101|1915-01-01|1   |1    |1915|Friday   |January   |false     |Winter|\n",
      "|19150303|1915-03-03|3   |3    |1915|Wednesday|March     |false     |Spring|\n",
      "|19160000|null      |null|null |1916|null     |null      |null      |null  |\n",
      "|19160905|1916-09-05|5   |9    |1916|Tuesday  |September |false     |Fall  |\n",
      "|19170000|null      |null|null |1917|null     |null      |null      |null  |\n",
      "|19180000|null      |null|null |1918|null     |null      |null      |null  |\n",
      "|19190000|null      |null|null |1919|null     |null      |null      |null  |\n",
      "|19190513|1919-05-13|13  |5    |1919|Tuesday  |May       |false     |Spring|\n",
      "|19191003|1919-10-03|3   |10   |1919|Friday   |October   |false     |Fall  |\n",
      "|19200000|null      |null|null |1920|null     |null      |null      |null  |\n",
      "|19200101|1920-01-01|1   |1    |1920|Thursday |January   |false     |Winter|\n",
      "|19200226|1920-02-26|26  |2    |1920|Thursday |February  |false     |Winter|\n",
      "|19200903|1920-09-03|3   |9    |1920|Friday   |September |false     |Fall  |\n",
      "+--------+----------+----+-----+----+---------+----------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dim_time\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "SILVER = \"hdfs://hdfs-nn:9000/demo/silver\"\n",
    "GOLD_DIM_TIME = \"hdfs://hdfs-nn:9000/demo/golds/dim_time\"\n",
    "\n",
    "# 1) Read real date sources\n",
    "piracy = spark.read.parquet(f\"{SILVER}/dataset_piracy\") \\\n",
    "    .select(\n",
    "        F.to_date(\"posted_date\").alias(\"posted_date\"),\n",
    "        F.to_date(\"release_date\").alias(\"release_date\")\n",
    "    )\n",
    "\n",
    "rt = spark.read.parquet(f\"{SILVER}/rotten_tomatoes_movies\") \\\n",
    "    .select(\n",
    "        F.to_date(\"in_theaters_date\").alias(\"in_theaters_date\"),\n",
    "        F.to_date(\"on_streaming_date\").alias(\"on_streaming_date\")\n",
    "    )\n",
    "\n",
    "boxinfo = spark.read.parquet(f\"{SILVER}/boxoffice_info\") \\\n",
    "    .select(F.to_date(F.col(\"released\")).alias(\"released\"))\n",
    "\n",
    "# 2) Collect all distinct real dates\n",
    "real_dates = (\n",
    "    piracy.select(F.col(\"posted_date\").alias(\"date\"))\n",
    "          .unionByName(piracy.select(F.col(\"release_date\").alias(\"date\")))\n",
    "          .unionByName(rt.select(F.col(\"in_theaters_date\").alias(\"date\")))\n",
    "          .unionByName(rt.select(F.col(\"on_streaming_date\").alias(\"date\")))\n",
    "          .unionByName(boxinfo.select(F.col(\"released\").alias(\"date\")))\n",
    "          .where(F.col(\"date\").isNotNull())\n",
    "          .distinct()\n",
    ")\n",
    "\n",
    "# 3) Daily grain (is_weekend as INT 0/1)\n",
    "dim_time_daily = (\n",
    "    real_dates\n",
    "    .withColumn(\"id_time\", F.date_format(\"date\", \"yyyyMMdd\").cast(\"int\"))\n",
    "    .withColumn(\"day\", F.dayofmonth(\"date\"))\n",
    "    .withColumn(\"month\", F.month(\"date\"))\n",
    "    .withColumn(\"year\", F.year(\"date\"))\n",
    "    .withColumn(\"day_name\", F.date_format(\"date\", \"EEEE\"))\n",
    "    .withColumn(\"month_name\", F.date_format(\"date\", \"MMMM\"))\n",
    "    .withColumn(\n",
    "        \"is_weekend\",\n",
    "        F.dayofweek(\"date\").isin(1, 7)  # devolve boolean\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"season\",\n",
    "        F.when(F.col(\"month\").isin([12, 1, 2]), \"Winter\")\n",
    "         .when(F.col(\"month\").isin([3, 4, 5]), \"Spring\")\n",
    "         .when(F.col(\"month\").isin([6, 7, 8]), \"Summer\")\n",
    "         .otherwise(\"Fall\")\n",
    "    )\n",
    "    .select(\"id_time\", \"date\", \"day\", \"month\", \"year\", \"day_name\", \"month_name\", \"is_weekend\", \"season\")\n",
    ")\n",
    "\n",
    "\n",
    "# 4) Year-only grain (1910..2024) - is_weekend NULL but INT type\n",
    "years_df = spark.range(1910, 2025).select(F.col(\"id\").cast(\"int\").alias(\"year\"))\n",
    "\n",
    "dim_time_year_only = (\n",
    "    years_df\n",
    "    .withColumn(\"id_time\", (F.col(\"year\") * 10000).cast(\"int\"))\n",
    "    .withColumn(\"date\", F.lit(None).cast(\"date\"))\n",
    "    .withColumn(\"day\", F.lit(None).cast(\"int\"))\n",
    "    .withColumn(\"month\", F.lit(None).cast(\"int\"))\n",
    "    .withColumn(\"day_name\", F.lit(None).cast(\"string\"))\n",
    "    .withColumn(\"month_name\", F.lit(None).cast(\"string\"))\n",
    "    .withColumn(\"is_weekend\", F.lit(None).cast(\"boolean\"))\n",
    "    .withColumn(\"season\", F.lit(None).cast(\"string\"))\n",
    "    .select(\"id_time\", \"date\", \"day\", \"month\", \"year\", \"day_name\", \"month_name\", \"is_weekend\", \"season\")\n",
    ")\n",
    "\n",
    "# 5) Union & write\n",
    "dim_time = (\n",
    "    dim_time_daily\n",
    "    .unionByName(dim_time_year_only)\n",
    "    .dropDuplicates([\"id_time\"])\n",
    ")\n",
    "\n",
    "dim_time.write.mode(\"overwrite\").parquet(GOLD_DIM_TIME)\n",
    "\n",
    "# Quick check\n",
    "dim_time.printSchema()\n",
    "dim_time.orderBy(F.col(\"id_time\").asc()).show(20, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04d5521c-6f95-4869-9d2f-55a4935311d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------------------+----------+-------+-----------+-----+\n",
      "|id_title|title                       |age_rating|runtime|studio_name|type |\n",
      "+--------+----------------------------+----------+-------+-----------+-----+\n",
      "|1       |#ABtalks                    |TV-PG     |68     |null       |show |\n",
      "|2       |#Alive                      |null      |98     |null       |movie|\n",
      "|3       |#AnneFrank. Parallel Stories|null      |92     |null       |movie|\n",
      "|4       |#FriendButMarried           |null      |102    |null       |movie|\n",
      "|5       |#FriendButMarried 2         |null      |104    |null       |movie|\n",
      "+--------+----------------------------+----------+-------+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "count: 9999\n"
     ]
    }
   ],
   "source": [
    "#dim_title\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "SILVER_TITLES = \"hdfs://hdfs-nn:9000/demo/silver/titles\"\n",
    "SILVER_RT     = \"hdfs://hdfs-nn:9000/demo/silver/rotten_tomatoes_movies\"\n",
    "\n",
    "titles = spark.read.parquet(SILVER_TITLES)\n",
    "\n",
    "rt = (\n",
    "    spark.read.parquet(SILVER_RT)\n",
    "      .select(\n",
    "          F.col(\"movie_title\").alias(\"rt_title\"),\n",
    "          F.col(\"studio_name\")\n",
    "      )\n",
    ")\n",
    "\n",
    "dim_title_df = (\n",
    "    titles.alias(\"t\")\n",
    "    .join(\n",
    "        rt.alias(\"r\"),\n",
    "        F.lower(F.col(\"t.title\")) == F.lower(F.col(\"r.rt_title\")),\n",
    "        \"left\"\n",
    "    )\n",
    "    .select(\n",
    "        F.col(\"t.id_title\").cast(\"int\").alias(\"id_title\"),\n",
    "        F.col(\"t.title\").alias(\"title\"),\n",
    "        F.when(F.col(\"t.age_certification\") == \"unknown\", None)\n",
    "         .otherwise(F.upper(F.col(\"t.age_certification\"))).alias(\"age_rating\"),\n",
    "        F.col(\"t.runtime\").cast(\"int\").alias(\"runtime\"),\n",
    "        F.col(\"r.studio_name\").alias(\"studio_name\"),\n",
    "        F.lower(F.col(\"t.type\")).alias(\"type\")\n",
    "    )\n",
    "    .dropDuplicates([\"id_title\"])\n",
    ")\n",
    "\n",
    "\n",
    "dim_title_df.write.mode(\"overwrite\").insertInto(\"golds.dim_title\")\n",
    "\n",
    "\n",
    "\n",
    "spark.table(\"golds.dim_title\").show(5, truncate=False)\n",
    "print(\"count:\", spark.table(\"golds.dim_title\").count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6613ca21-7acd-43e6-996d-472a4d5cb4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+------------------+---------------+\n",
      "|id_rating_source_status|tomatometer_status|source         |\n",
      "+-----------------------+------------------+---------------+\n",
      "|1                      |score             |imdb           |\n",
      "|2                      |certified fresh   |rotten_tomatoes|\n",
      "|3                      |fresh             |rotten_tomatoes|\n",
      "|4                      |rotten            |rotten_tomatoes|\n",
      "|5                      |score             |tmdb           |\n",
      "+-----------------------+------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dim_rating_status\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "HDFS_NN = \"hdfs://hdfs-nn:9000\"\n",
    "SILVER  = f\"{HDFS_NN}/demo/silver\"\n",
    "GOLD    = f\"{HDFS_NN}/demo/golds\"\n",
    "\n",
    "RT_SILVER  = f\"{SILVER}/rotten_tomatoes_movies\"\n",
    "DRS_PATH   = f\"{GOLD}/dim_rating_status\"\n",
    "\n",
    "# -------- 1) Rotten Tomatoes: status + source ----------\n",
    "rt = spark.read.parquet(RT_SILVER)\n",
    "\n",
    "rt_status = (\n",
    "    rt.select(F.col(\"tomatometer_status\").alias(\"status\"))\n",
    "      .filter(F.col(\"status\").isNotNull())\n",
    "      .withColumn(\"status\", F.trim(F.col(\"status\")))\n",
    "      .dropDuplicates([\"status\"])\n",
    "      .withColumn(\"source\", F.lit(\"rotten_tomatoes\"))\n",
    ")\n",
    "\n",
    "# -------- 2) IMDB / TMDB linhas extra ----------\n",
    "extra_status = spark.createDataFrame(\n",
    "    [\n",
    "        (\"score\", \"imdb\"),\n",
    "        (\"score\", \"tmdb\"),\n",
    "    ],\n",
    "    [\"status\", \"source\"]\n",
    ")\n",
    "\n",
    "# -------- 3) União + normalização ----------\n",
    "all_status = (\n",
    "    rt_status\n",
    "    .unionByName(extra_status)\n",
    "    .withColumn(\"status\", F.lower(F.trim(\"status\")))\n",
    "    .withColumn(\"source\", F.lower(F.trim(\"source\")))\n",
    "    .dropDuplicates([\"status\", \"source\"])\n",
    ")\n",
    "\n",
    "# -------- 4) Surrogate key estável ----------\n",
    "dim_rating_status = (\n",
    "    all_status\n",
    "    .withColumn(\n",
    "        \"id_rating_source_status\",\n",
    "        F.row_number().over(Window.orderBy(\"source\", \"status\")).cast(\"bigint\")\n",
    "    )\n",
    "    .select(\n",
    "        \"id_rating_source_status\",\n",
    "        F.col(\"status\").alias(\"tomatometer_status\"),\n",
    "        \"source\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# -------- 5) Escrever Gold ----------\n",
    "(\n",
    "    dim_rating_status\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"parquet\")\n",
    "    .option(\"path\", DRS_PATH)\n",
    "    .saveAsTable(\"golds.dim_rating_status\")\n",
    ")\n",
    "\n",
    "dim_rating_status.orderBy(\"source\", \"tomatometer_status\").show(100, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36d4c898-dd46-4225-94b1-cfc64496173f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------------------------------------+\n",
      "|id_award|canonical_category                               |\n",
      "+--------+-------------------------------------------------+\n",
      "|1       |actor in a leading role                          |\n",
      "|2       |actor in a supporting role                       |\n",
      "|3       |actress in a leading role                        |\n",
      "|4       |actress in a supporting role                     |\n",
      "|5       |animated feature film                            |\n",
      "|6       |art direction                                    |\n",
      "|7       |art direction (black-and-white)                  |\n",
      "|8       |art direction (color)                            |\n",
      "|9       |assistant director                               |\n",
      "|10      |award of commendation                            |\n",
      "|11      |best picture                                     |\n",
      "|12      |cinematography                                   |\n",
      "|13      |cinematography (black-and-white)                 |\n",
      "|14      |cinematography (color)                           |\n",
      "|15      |costume design                                   |\n",
      "|16      |costume design (black-and-white)                 |\n",
      "|17      |costume design (color)                           |\n",
      "|18      |dance direction                                  |\n",
      "|19      |directing                                        |\n",
      "|20      |directing (comedy picture)                       |\n",
      "|21      |directing (dramatic picture)                     |\n",
      "|22      |documentary (feature)                            |\n",
      "|23      |documentary (short subject)                      |\n",
      "|24      |film editing                                     |\n",
      "|25      |gordon e. sawyer award                           |\n",
      "|26      |honorary award                                   |\n",
      "|27      |international feature film                       |\n",
      "|28      |irving g. thalberg memorial award                |\n",
      "|29      |jean hersholt humanitarian award                 |\n",
      "|30      |john a. bonner medal of commendation             |\n",
      "|31      |makeup and hairstyling                           |\n",
      "|32      |medal of commendation                            |\n",
      "|33      |music (original score)                           |\n",
      "|34      |music (original song score or adaptation score)  |\n",
      "|35      |music (original song)                            |\n",
      "|36      |short film (animated)                            |\n",
      "|37      |short film (live action)                         |\n",
      "|38      |short subject (color)                            |\n",
      "|39      |short subject (comedy)                           |\n",
      "|40      |short subject (novelty)                          |\n",
      "|41      |short subject (one-reel)                         |\n",
      "|42      |short subject (two-reel)                         |\n",
      "|43      |sound editing                                    |\n",
      "|44      |sound mixing                                     |\n",
      "|45      |sound recording                                  |\n",
      "|46      |special achievement award                        |\n",
      "|47      |special achievement award (sound editing)        |\n",
      "|48      |special achievement award (sound effects editing)|\n",
      "|49      |special achievement award (sound effects)        |\n",
      "|50      |special achievement award (visual effects)       |\n",
      "|51      |special award                                    |\n",
      "|52      |special foreign language film award              |\n",
      "|53      |unique and artistic picture                      |\n",
      "|54      |visual effects                                   |\n",
      "|55      |writing (adapted screenplay)                     |\n",
      "|56      |writing (original screenplay)                    |\n",
      "|57      |writing (original story)                         |\n",
      "|58      |writing (title writing)                          |\n",
      "+--------+-------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dim_category  \n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "OSCARS = spark.read.parquet(\"hdfs://hdfs-nn:9000/demo/silver/the_oscar_award\")\n",
    "\n",
    "dim_category = (\n",
    "    OSCARS\n",
    "    .select(F.trim(F.col(\"canon_category\")).alias(\"canonical_category\"))\n",
    "    .where(F.col(\"canonical_category\").isNotNull())\n",
    "    .dropDuplicates([\"canonical_category\"])\n",
    "    .withColumn(\n",
    "        \"id_award\",\n",
    "        F.row_number().over(Window.orderBy(F.lower(F.col(\"canonical_category\"))))\n",
    "    )\n",
    "    .select(\"id_award\", \"canonical_category\")\n",
    ")\n",
    "\n",
    "(dim_category.write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"parquet\")\n",
    "    .option(\"path\", \"hdfs://hdfs-nn:9000/demo/golds/dim_category\")\n",
    "    .saveAsTable(\"golds.dim_category\"))\n",
    "\n",
    "dim_category.orderBy(\"id_award\").show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6cbf1fe-299c-4e8c-b8ef-76afaec47b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+-----------+\n",
      "|id_person|name              |person_type|\n",
      "+---------+------------------+-----------+\n",
      "|1        |21 Savage         |actor      |\n",
      "|2        |2Mex              |actor      |\n",
      "|3        |50 Cent           |actor      |\n",
      "|4        |50-Grand          |actor      |\n",
      "|5        |87gongzhu         |actor      |\n",
      "|6        |9m88              |actor      |\n",
      "|7        |A Leslie Kies     |actor      |\n",
      "|8        |A Martinez        |actor      |\n",
      "|9        |A$AP Rocky        |actor      |\n",
      "|10       |A. Ali Flores     |actor      |\n",
      "|11       |A. Bernard Sneed  |actor      |\n",
      "|12       |A. C. Murali Mohan|actor      |\n",
      "|13       |A. E. Manoharan   |actor      |\n",
      "|14       |A. Joseph Denucci |actor      |\n",
      "|15       |A. Karunanidhi    |actor      |\n",
      "|16       |A. Murat Özgen    |actor      |\n",
      "|17       |A. R. Manikandan  |actor      |\n",
      "|18       |A. R. Rahman      |actor      |\n",
      "|19       |A. Smith Harrison |actor      |\n",
      "|20       |A. V. M. Rajan    |actor      |\n",
      "|21       |A. V. Ramanan     |actor      |\n",
      "|22       |A. Venkatesh      |actor      |\n",
      "|23       |A.A. Loyd         |actor      |\n",
      "|24       |A.B. Cassidy      |actor      |\n",
      "|25       |A.B. Greeson      |actor      |\n",
      "|26       |A.D. Miles        |actor      |\n",
      "|27       |A.D. Muyich       |actor      |\n",
      "|28       |A.J. Ackleson     |actor      |\n",
      "|29       |A.J. Bernardo     |actor      |\n",
      "|30       |A.J. Buckley      |actor      |\n",
      "|31       |A.J. Cook         |actor      |\n",
      "|32       |A.J. Freeman      |actor      |\n",
      "|33       |A.J. Henderson    |actor      |\n",
      "|34       |A.J. Houghton     |actor      |\n",
      "|35       |A.J. Hudson       |actor      |\n",
      "|36       |A.J. Jarrell      |actor      |\n",
      "|37       |A.J. Jenks        |actor      |\n",
      "|38       |A.J. Johnson      |actor      |\n",
      "|39       |A.J. Khan         |actor      |\n",
      "|40       |A.J. Little John  |actor      |\n",
      "|41       |A.J. LoCascio     |actor      |\n",
      "|42       |A.J. Rivera       |actor      |\n",
      "|43       |A.J. Saudin       |actor      |\n",
      "|44       |A.J. Seims        |actor      |\n",
      "|45       |A.J. Tannen       |actor      |\n",
      "|46       |A.J. Trauth       |actor      |\n",
      "|47       |A.K. Hangal       |actor      |\n",
      "|48       |A.K. Moore        |actor      |\n",
      "|49       |A.L. Schaeffer    |actor      |\n",
      "|50       |A.R. Bala         |actor      |\n",
      "+---------+------------------+-----------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dim_person\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "HDFS_NN = \"hdfs://hdfs-nn:9000\"\n",
    "SILVER_BASE = f\"{HDFS_NN}/demo/silver\"\n",
    "GOLD_BASE   = f\"{HDFS_NN}/demo/golds\"\n",
    "\n",
    "CREDITS_PATH = f\"{SILVER_BASE}/credits\"\n",
    "RT_PATH      = f\"{SILVER_BASE}/rotten_tomatoes_movies\"\n",
    "PIRACY_PATH  = f\"{SILVER_BASE}/dataset_piracy\"\n",
    "\n",
    "OUT_PATH  = f\"{GOLD_BASE}/dim_person\"\n",
    "OUT_TABLE = \"golds.dim_person\"\n",
    "\n",
    "credits = spark.read.parquet(CREDITS_PATH)\n",
    "rt      = spark.read.parquet(RT_PATH)\n",
    "piracy  = spark.read.parquet(PIRACY_PATH)\n",
    "\n",
    "def clean_person_name(col_):\n",
    "    x = F.trim(col_)\n",
    "    x = F.regexp_replace(x, r\"\\s+\", \" \")\n",
    "    x = F.regexp_replace(x, r'^[\\'\"]+|[\\'\"]+$', \"\")\n",
    "    x = F.when(\n",
    "        (x.isNull()) | (F.length(x) == 0) |\n",
    "        (F.lower(x).isin(\"missing\", \"missingunknown\", \"unknown\", \"null\", \"none\", \"n/a\")),\n",
    "        F.lit(None).cast(\"string\")\n",
    "    ).otherwise(x)\n",
    "    return x\n",
    "\n",
    "def split_csv(col_):\n",
    "    return F.split(col_, r\"\\s*,\\s*\")\n",
    "\n",
    "# 1) actors + directors from credits (role column)\n",
    "people_credits = (\n",
    "    credits\n",
    "    .select(\n",
    "        clean_person_name(F.col(\"name\")).alias(\"name\"),\n",
    "        F.upper(F.trim(F.col(\"role\"))).alias(\"role\")\n",
    "    )\n",
    "    .where(F.col(\"name\").isNotNull())\n",
    "    .where(F.col(\"role\").isin(\"ACTOR\", \"DIRECTOR\"))\n",
    "    .select(\n",
    "        F.col(\"name\"),\n",
    "        F.when(F.col(\"role\") == \"ACTOR\", F.lit(\"actor\"))\n",
    "         .otherwise(F.lit(\"director\"))\n",
    "         .alias(\"person_type\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# 2) writers from RT writers\n",
    "people_rt_writers = (\n",
    "    rt.select(F.col(\"writers\").alias(\"raw\"))\n",
    "      .where(F.col(\"raw\").isNotNull())\n",
    "      .withColumn(\"arr\", split_csv(F.col(\"raw\")))\n",
    "      .withColumn(\"name\", F.explode(F.col(\"arr\")))      # explode first (safe) [file:4]\n",
    "      .select(clean_person_name(F.col(\"name\")).alias(\"name\"))\n",
    "      .where(F.col(\"name\").isNotNull())\n",
    "      .withColumn(\"person_type\", F.lit(\"writer\"))\n",
    ")\n",
    "\n",
    "# 3) directors from RT directors\n",
    "people_rt_directors = (\n",
    "    rt.select(F.col(\"directors\").alias(\"raw\"))\n",
    "      .where(F.col(\"raw\").isNotNull())\n",
    "      .withColumn(\"arr\", split_csv(F.col(\"raw\")))\n",
    "      .withColumn(\"name\", F.explode(F.col(\"arr\")))\n",
    "      .select(clean_person_name(F.col(\"name\")).alias(\"name\"))\n",
    "      .where(F.col(\"name\").isNotNull())\n",
    "      .withColumn(\"person_type\", F.lit(\"director\"))\n",
    ")\n",
    "\n",
    "# 4) writers from piracy writer\n",
    "people_piracy_writers = (\n",
    "    piracy.select(F.col(\"writer\").alias(\"raw\"))\n",
    "          .where(F.col(\"raw\").isNotNull())\n",
    "          .withColumn(\"arr\", split_csv(F.col(\"raw\")))\n",
    "          .withColumn(\"name\", F.explode(F.col(\"arr\")))\n",
    "          .select(clean_person_name(F.col(\"name\")).alias(\"name\"))\n",
    "          .where(F.col(\"name\").isNotNull())\n",
    "          .withColumn(\"person_type\", F.lit(\"writer\"))\n",
    ")\n",
    "\n",
    "# 5) directors from piracy director\n",
    "people_piracy_directors = (\n",
    "    piracy.select(F.col(\"director\").alias(\"raw\"))\n",
    "          .where(F.col(\"raw\").isNotNull())\n",
    "          .withColumn(\"arr\", split_csv(F.col(\"raw\")))\n",
    "          .withColumn(\"name\", F.explode(F.col(\"arr\")))\n",
    "          .select(clean_person_name(F.col(\"name\")).alias(\"name\"))\n",
    "          .where(F.col(\"name\").isNotNull())\n",
    "          .withColumn(\"person_type\", F.lit(\"director\"))\n",
    ")\n",
    "\n",
    "# Union all -> dimperson\n",
    "dim_person_base = (\n",
    "    people_credits\n",
    "    .unionByName(people_rt_writers)\n",
    "    .unionByName(people_rt_directors)\n",
    "    .unionByName(people_piracy_writers)\n",
    "    .unionByName(people_piracy_directors)\n",
    "    .dropDuplicates([\"name\", \"person_type\"])  # same name can exist in multiple roles [file:4]\n",
    ")\n",
    "\n",
    "w = Window.orderBy(F.col(\"person_type\").asc(), F.col(\"name\").asc())\n",
    "dim_person = (\n",
    "    dim_person_base\n",
    "    .withColumn(\"id_person\", F.row_number().over(w).cast(\"int\"))\n",
    "    .select(\"id_person\", \"name\", \"person_type\")\n",
    ")\n",
    "\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {OUT_TABLE}\")\n",
    "\n",
    "(\n",
    "    dim_person.write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"parquet\")\n",
    "    .option(\"path\", OUT_PATH)\n",
    "    .saveAsTable(OUT_TABLE)\n",
    ")\n",
    "\n",
    "dim_person.orderBy(\"id_person\").show(50, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10806794-7b11-47e8-8540-cf214e9cd025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------------+\n",
      "|id_genre|genre                    |\n",
      "+--------+-------------------------+\n",
      "|1       |action                   |\n",
      "|2       |action & adventure       |\n",
      "|3       |animation                |\n",
      "|4       |anime & manga            |\n",
      "|5       |art house & international|\n",
      "|6       |classics                 |\n",
      "|7       |comedy                   |\n",
      "|8       |crime                    |\n",
      "|9       |cult movies              |\n",
      "|10      |documentary              |\n",
      "|11      |drama                    |\n",
      "|12      |european                 |\n",
      "|13      |faith & spirituality     |\n",
      "|14      |family                   |\n",
      "|15      |fantasy                  |\n",
      "|16      |gay & lesbian            |\n",
      "|17      |history                  |\n",
      "|18      |horror                   |\n",
      "|19      |kids & family            |\n",
      "|20      |music                    |\n",
      "|21      |musical & performing arts|\n",
      "|22      |mystery & suspense       |\n",
      "|23      |reality                  |\n",
      "|24      |romance                  |\n",
      "|25      |science fiction & fantasy|\n",
      "|26      |scifi                    |\n",
      "|27      |sport                    |\n",
      "|28      |sports & fitness         |\n",
      "|29      |television               |\n",
      "|30      |thriller                 |\n",
      "|31      |war                      |\n",
      "|32      |western                  |\n",
      "+--------+-------------------------+\n",
      "\n",
      "+---+----+\n",
      "|  n|n_id|\n",
      "+---+----+\n",
      "| 32|  32|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dim_genre\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "HDFS_NN     = \"hdfs://hdfs-nn:9000\"\n",
    "SILVER_BASE = f\"{HDFS_NN}/demo/silver\"\n",
    "GOLD_BASE   = f\"{HDFS_NN}/demo/golds\"\n",
    "\n",
    "TITLES_PATH = f\"{SILVER_BASE}/titles\"\n",
    "RT_PATH     = f\"{SILVER_BASE}/rotten_tomatoes_movies\"\n",
    "OUT_PATH    = f\"{GOLD_BASE}/dim_genre\"\n",
    "OUT_TABLE   = \"golds.dim_genre\"\n",
    "\n",
    "def kt(col_):\n",
    "    return F.lower(F.trim(col_))\n",
    "\n",
    "# -------------------------\n",
    "# 0) Lê fontes\n",
    "# -------------------------\n",
    "titles = spark.read.parquet(TITLES_PATH)\n",
    "rt     = spark.read.parquet(RT_PATH)\n",
    "\n",
    "# -------------------------\n",
    "# 1) Extrai géneros normalizados de titles\n",
    "#    - genres pode vir como \"['a','b']\" OU \"a, b\"\n",
    "# -------------------------\n",
    "titles_genres = (\n",
    "    titles\n",
    "    .select(F.col(\"genres\").alias(\"raw\"))\n",
    "    .filter(F.col(\"raw\").isNotNull())\n",
    "    .withColumn(\n",
    "        \"arr\",\n",
    "        F.when(\n",
    "            F.col(\"raw\").rlike(r\"^\\s*\\[.*\\]\\s*$\"),\n",
    "            F.from_json(F.regexp_replace(F.col(\"raw\"), r\"'\", '\"'), \"array<string>\")\n",
    "        ).otherwise(\n",
    "            F.split(F.col(\"raw\"), r\"\\s*,\\s*\")\n",
    "        )\n",
    "    )\n",
    "    .select(F.explode(\"arr\").alias(\"genero\"))\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 2) Extrai géneros normalizados de Rotten (genre = \"a, b\")\n",
    "# -------------------------\n",
    "rt_genres = (\n",
    "    rt\n",
    "    .select(F.col(\"genre\").alias(\"raw\"))\n",
    "    .filter(F.col(\"raw\").isNotNull())\n",
    "    .withColumn(\"arr\", F.split(F.col(\"raw\"), r\"\\s*,\\s*\"))\n",
    "    .select(F.explode(\"arr\").alias(\"genero\"))\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 3) União + limpeza básica\n",
    "# -------------------------\n",
    "dim_genre_raw = (\n",
    "    titles_genres\n",
    "    .unionByName(rt_genres)\n",
    "    .select(kt(F.col(\"genero\")).alias(\"genre\"))\n",
    "    .filter(F.col(\"genre\").isNotNull())\n",
    "    .filter(F.col(\"genre\") != \"\")\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 4) Mapeamento de sinónimos / aliases\n",
    "# -------------------------\n",
    "from itertools import chain\n",
    "\n",
    "# Dicionário de sinónimos → alvo normalizado\n",
    "synonyms = {\n",
    "    \"documentation\": \"documentary\",\n",
    "    \"special interest\": \"documentary\",\n",
    "    # acrescenta aqui outros: \"sci-fi\" -> \"science fiction\", etc.\n",
    "}\n",
    "\n",
    "# Expressão de mapping: create_map('documentation','documentary', ...)\n",
    "mapping_expr = F.create_map(\n",
    "    [F.lit(x) for x in chain(*synonyms.items())]\n",
    ")\n",
    "\n",
    "dim_genre_norm = (\n",
    "    dim_genre_raw\n",
    "    .withColumn(\n",
    "        \"genre_norm\",\n",
    "        F.coalesce(mapping_expr[F.col(\"genre\")], F.col(\"genre\"))\n",
    "    )\n",
    "    .select(\"genre_norm\")\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 5) Surrogate key e seleção final\n",
    "# -------------------------\n",
    "dim_genre = (\n",
    "    dim_genre_norm\n",
    "    .withColumn(\n",
    "        \"id_genre\",\n",
    "        F.row_number().over(Window.orderBy(\"genre_norm\")).cast(\"int\")\n",
    "    )\n",
    "    .select(\"id_genre\", F.col(\"genre_norm\").alias(\"genre\"))\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 6) Write Gold (Parquet + tabela)\n",
    "# -------------------------\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {OUT_TABLE}\")\n",
    "\n",
    "(\n",
    "    dim_genre.write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"parquet\")\n",
    "    .option(\"path\", OUT_PATH)\n",
    "    .saveAsTable(OUT_TABLE)\n",
    ")\n",
    "\n",
    "dim_genre.orderBy(\"id_genre\").show(50, truncate=False)\n",
    "spark.sql(f\"SELECT COUNT(*) AS n, COUNT(id_genre) AS n_id FROM {OUT_TABLE}\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a7fe918-53eb-4bca-b477-a879c688f94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|id_platform|platform_name|\n",
      "+-----------+-------------+\n",
      "|1          |amazon       |\n",
      "|2          |netflix      |\n",
      "+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dim_platform\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "HDFS_NN   = \"hdfs://hdfs-nn:9000\"\n",
    "GOLD_BASE = f\"{HDFS_NN}/demo/golds\"\n",
    "\n",
    "OUT_PATH  = f\"{GOLD_BASE}/dim_platform\"\n",
    "OUT_TABLE = \"golds.dim_platform\"\n",
    "\n",
    "# Pequena lista estática de plataformas atuais\n",
    "platforms = spark.createDataFrame(\n",
    "    [\n",
    "        Row(platform_name=\"netflix\"),\n",
    "        Row(platform_name=\"amazon\")\n",
    "        # se no futuro tiveres mais: Row(platform_name=\"disney\"), Row(platform_name=\"hbo\"), ...\n",
    "    ]\n",
    ")\n",
    "\n",
    "dim_platform = (\n",
    "    platforms\n",
    "    .withColumn(\"platform_name\", F.trim(F.lower(\"platform_name\")))\n",
    "    .dropDuplicates([\"platform_name\"])\n",
    "    .withColumn(\n",
    "        \"id_platform\",\n",
    "        F.row_number().over(Window.orderBy(\"platform_name\")).cast(\"int\")\n",
    "    )\n",
    "    .select(\"id_platform\", \"platform_name\")\n",
    ")\n",
    "\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {OUT_TABLE}\")\n",
    "\n",
    "(\n",
    "    dim_platform.write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"parquet\")\n",
    "    .option(\"path\", OUT_PATH)\n",
    "    .saveAsTable(OUT_TABLE)\n",
    ")\n",
    "\n",
    "dim_platform.orderBy(\"id_platform\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e9dacbf-6456-4801-8525-940bf9661a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|id_group_genre|count|\n",
      "+--------------+-----+\n",
      "|890           |11   |\n",
      "|889           |11   |\n",
      "|1014          |11   |\n",
      "|170           |11   |\n",
      "|1024          |11   |\n",
      "|174           |11   |\n",
      "|412           |10   |\n",
      "|891           |10   |\n",
      "|173           |10   |\n",
      "|176           |10   |\n",
      "+--------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------+--------+\n",
      "|id_group_genre|id_genre|\n",
      "+--------------+--------+\n",
      "|1             |1       |\n",
      "|2             |1       |\n",
      "|2             |10      |\n",
      "|3             |1       |\n",
      "|3             |10      |\n",
      "|3             |11      |\n",
      "|3             |12      |\n",
      "|3             |32      |\n",
      "|4             |1       |\n",
      "|4             |10      |\n",
      "+--------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#bridge_launch_genre\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "HDFS_NN     = \"hdfs://hdfs-nn:9000\"\n",
    "SILVER_BASE = f\"{HDFS_NN}/demo/silver\"\n",
    "GOLD_BASE   = f\"{HDFS_NN}/demo/golds\"\n",
    "\n",
    "# INPUTS\n",
    "DIM_TITLE_PATH = f\"{GOLD_BASE}/dim_title\"\n",
    "DIM_GENRE_PATH = f\"{GOLD_BASE}/dim_genre\"\n",
    "\n",
    "TITLES_PATH = f\"{SILVER_BASE}/titles\"\n",
    "RT_PATH     = f\"{SILVER_BASE}/rotten_tomatoes_movies\"\n",
    "\n",
    "# OUTPUTS\n",
    "OUT_BRIDGE = f\"{GOLD_BASE}/bridge_launch_genre\"\n",
    "OUT_MAP    = f\"{GOLD_BASE}/map_title_group_genre\"     # para alimentar a fact launch\n",
    "# (se não quiseres o mapa, podes apagar esta linha e o write correspondente)\n",
    "\n",
    "def kt(col_):\n",
    "    return F.lower(F.trim(col_))\n",
    "\n",
    "# -------------------------\n",
    "# 0) Read dims + sources\n",
    "# -------------------------\n",
    "dim_title = spark.read.parquet(DIM_TITLE_PATH)   # (idtitle, title, ...)\n",
    "dim_genre = spark.read.parquet(DIM_GENRE_PATH)   # (id_genre, genre)\n",
    "\n",
    "titles = spark.read.parquet(TITLES_PATH)         # tem title + genres (raw)\n",
    "rt     = spark.read.parquet(RT_PATH)             # tem movie_title + genre (raw)\n",
    "\n",
    "# -------------------------\n",
    "# 1) Build title -> genre_name pairs (normalizados)\n",
    "#    - titles.genres pode vir como \"['a','b']\" ou \"a, b\"\n",
    "#    - rt.genre vem como \"a, b\"\n",
    "# -------------------------\n",
    "titles_pairs = (\n",
    "    titles\n",
    "    .select(F.col(\"title\").alias(\"title_raw\"), F.col(\"genres\").alias(\"raw\"))\n",
    "    .where(F.col(\"title_raw\").isNotNull() & F.col(\"raw\").isNotNull())\n",
    "    .withColumn(\"k_title\", kt(F.col(\"title_raw\")))\n",
    "    .withColumn(\n",
    "        \"arr\",\n",
    "        F.when(\n",
    "            F.col(\"raw\").rlike(r\"^\\s*\\[.*\\]\\s*$\"),\n",
    "            F.from_json(F.regexp_replace(F.col(\"raw\"), r\"'\", '\"'), \"array<string>\")\n",
    "        ).otherwise(\n",
    "            F.split(F.col(\"raw\"), r\"\\s*,\\s*\")\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\"genre_name\", F.explode(F.col(\"arr\")))\n",
    "    .withColumn(\"genre_name\", kt(F.col(\"genre_name\")))\n",
    "    .select(\"k_title\", \"genre_name\")\n",
    "    .where(F.col(\"k_title\").isNotNull() & (F.col(\"genre_name\") != \"\"))\n",
    ")\n",
    "\n",
    "rt_pairs = (\n",
    "    rt\n",
    "    .select(F.col(\"movie_title\").alias(\"title_raw\"), F.col(\"genre\").alias(\"raw\"))\n",
    "    .where(F.col(\"title_raw\").isNotNull() & F.col(\"raw\").isNotNull())\n",
    "    .withColumn(\"k_title\", kt(F.col(\"title_raw\")))\n",
    "    .withColumn(\"arr\", F.split(F.col(\"raw\"), r\"\\s*,\\s*\"))\n",
    "    .withColumn(\"genre_name\", F.explode(F.col(\"arr\")))\n",
    "    .withColumn(\"genre_name\", kt(F.col(\"genre_name\")))\n",
    "    .select(\"k_title\", \"genre_name\")\n",
    "    .where(F.col(\"k_title\").isNotNull() & (F.col(\"genre_name\") != \"\"))\n",
    ")\n",
    "\n",
    "title_genres = titles_pairs.unionByName(rt_pairs).dropDuplicates([\"k_title\", \"genre_name\"])\n",
    "\n",
    "# -------------------------\n",
    "# 2) Lookups via DIMs\n",
    "# -------------------------\n",
    "dim_title_lu = (\n",
    "    dim_title\n",
    "    .select(F.col(\"id_title\").alias(\"id_title\"), kt(F.col(\"title\")).alias(\"k_title\"))\n",
    "    .dropDuplicates([\"k_title\"])\n",
    ")\n",
    "\n",
    "dim_genre_lu = (\n",
    "    dim_genre\n",
    "    .select(\"id_genre\", kt(F.col(\"genre\")).alias(\"genre_name\"))\n",
    "    .dropDuplicates([\"genre_name\"])\n",
    ")\n",
    "\n",
    "title_genres_ids = (\n",
    "    title_genres\n",
    "    .join(dim_title_lu, \"k_title\", \"inner\")\n",
    "    .join(dim_genre_lu, \"genre_name\", \"inner\")\n",
    "    .select(\"id_title\", \"id_genre\")\n",
    "    .dropDuplicates()\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 3) Criar id_group_genre por \"conjunto de géneros\" do título\n",
    "# -------------------------\n",
    "sig = (\n",
    "    title_genres_ids\n",
    "    .groupBy(\"id_title\")\n",
    "    .agg(F.sort_array(F.collect_set(\"id_genre\")).alias(\"genres_set\"))\n",
    "    .withColumn(\"group_key\", F.concat_ws(\"-\", F.col(\"genres_set\")))\n",
    "    .select(\"id_title\", \"group_key\", \"genres_set\")\n",
    ")\n",
    "\n",
    "group_dim = (\n",
    "    sig.select(\"group_key\")\n",
    "    .dropDuplicates()\n",
    "    .withColumn(\"id_group_genre\", F.dense_rank().over(Window.orderBy(\"group_key\")).cast(\"int\"))\n",
    ")\n",
    "\n",
    "title_to_group_genre = (\n",
    "    sig.join(group_dim, \"group_key\", \"inner\")\n",
    "    .select(\"id_title\", \"id_group_genre\", \"genres_set\")\n",
    ")\n",
    "\n",
    "bridge_launch_genre = (\n",
    "    title_to_group_genre\n",
    "    .select(\"id_group_genre\", F.explode(\"genres_set\").alias(\"id_genre\"))\n",
    "    .select(\"id_group_genre\", \"id_genre\")\n",
    "    .dropDuplicates()\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 4) Write Gold\n",
    "# -------------------------\n",
    "bridge_launch_genre.write.mode(\"overwrite\").parquet(OUT_BRIDGE)\n",
    "\n",
    "# mapa para preencher launch.id_group_genre (join por id_title)\n",
    "title_to_group_genre.select(\"id_title\", \"id_group_genre\").write.mode(\"overwrite\").parquet(OUT_MAP)\n",
    "\n",
    "# quick checks\n",
    "spark.read.parquet(OUT_BRIDGE).groupBy(\"id_group_genre\").count().orderBy(F.desc(\"count\")).show(10, truncate=False)\n",
    "spark.read.parquet(OUT_BRIDGE).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc408172-9f00-4367-a81f-7f5329208751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+-----------+\n",
      "|id_genre|id_group_genre|genre      |\n",
      "+--------+--------------+-----------+\n",
      "|1       |2             |action     |\n",
      "|10      |2             |documentary|\n",
      "+--------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    " dim_genre = spark.read.parquet(\"hdfs://hdfs-nn:9000/demo/golds/dim_genre\")\n",
    "\n",
    "(\n",
    "    spark.read.parquet(\"hdfs://hdfs-nn:9000/demo/golds/bridge_launch_genre\")\n",
    "    .join(dim_genre, \"id_genre\", \"left\")\n",
    "    .where(F.col(\"id_group_genre\") == 2)  # por exemplo\n",
    "    .orderBy(\"id_genre\")\n",
    "    .show(truncate=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cf70228-6b21-4da4-b344-1a0d82555216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+\n",
      "|id_group_platform|id_platform|\n",
      "+-----------------+-----------+\n",
      "|2                |2          |\n",
      "|1                |1          |\n",
      "|2                |1          |\n",
      "|3                |2          |\n",
      "+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#bridge_views_platform\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "HDFS_NN     = \"hdfs://hdfs-nn:9000\"\n",
    "SILVER_BASE = f\"{HDFS_NN}/demo/silver\"\n",
    "GOLD_BASE   = f\"{HDFS_NN}/demo/golds\"\n",
    "\n",
    "DIM_TITLE_PATH    = f\"{GOLD_BASE}/dim_title\"\n",
    "DIM_PLATFORM_PATH = f\"{GOLD_BASE}/dim_platform\"\n",
    "TITLES_PATH       = f\"{SILVER_BASE}/titles\"\n",
    "\n",
    "OUT_BRIDGE = f\"{GOLD_BASE}/bridge_views_platform\"\n",
    "OUT_MAP    = f\"{GOLD_BASE}/map_title_group_platform\"\n",
    "\n",
    "def kt(c):\n",
    "    return F.lower(F.trim(c))\n",
    "\n",
    "dim_title    = spark.read.parquet(DIM_TITLE_PATH)       # id_title, title, ...\n",
    "dim_platform = spark.read.parquet(DIM_PLATFORM_PATH)    # id_platform, platform_name\n",
    "titles       = spark.read.parquet(TITLES_PATH)          # title, netflix, amazon, ...\n",
    "\n",
    "# 1) title -> platform_name (netflix/amazon flags)\n",
    "title_platforms = (\n",
    "    titles\n",
    "    .select(\n",
    "        kt(F.col(\"title\")).alias(\"k_title\"),\n",
    "        F.col(\"netflix\").cast(\"int\").alias(\"netflix\"),\n",
    "        F.col(\"amazon\").cast(\"int\").alias(\"amazon\")\n",
    "    )\n",
    "    .where(F.col(\"k_title\").isNotNull())\n",
    "    .withColumn(\n",
    "        \"platform_arr\",\n",
    "        F.array(\n",
    "            F.when(F.col(\"netflix\") == 1, F.lit(\"netflix\")),\n",
    "            F.when(F.col(\"amazon\")  == 1, F.lit(\"amazon\"))\n",
    "        )\n",
    "    )\n",
    "    # array_compact remove NULLs do array de plataformas\n",
    "    .withColumn(\"platform_arr\", F.array_compact(\"platform_arr\"))  # Spark 3.4+ / Databricks[web:166][web:170]\n",
    "    .withColumn(\"platform_name\", F.explode(\"platform_arr\"))\n",
    "    .select(\"k_title\", kt(F.col(\"platform_name\")).alias(\"platform_name\"))\n",
    "    .dropDuplicates()\n",
    ")\n",
    "\n",
    "# 2) lookups\n",
    "dim_title_lu = (\n",
    "    dim_title\n",
    "    .select(F.col(\"id_title\").alias(\"idtitle\"), kt(F.col(\"title\")).alias(\"k_title\"))\n",
    "    .dropDuplicates([\"k_title\"])\n",
    ")\n",
    "\n",
    "dim_platform_lu = (\n",
    "    dim_platform\n",
    "    .select(\"id_platform\", kt(F.col(\"platform_name\")).alias(\"platform_name\"))\n",
    "    .dropDuplicates([\"platform_name\"])\n",
    ")\n",
    "\n",
    "title_platforms_ids = (\n",
    "    title_platforms\n",
    "    .join(dim_title_lu, \"k_title\", \"inner\")\n",
    "    .join(dim_platform_lu, \"platform_name\", \"inner\")\n",
    "    .select(\"idtitle\", \"id_platform\")\n",
    "    .dropDuplicates()\n",
    ")\n",
    "\n",
    "# 3) Agrupar por conjunto de plataformas\n",
    "sig = (\n",
    "    title_platforms_ids\n",
    "    .groupBy(\"idtitle\")\n",
    "    .agg(F.sort_array(F.collect_set(\"id_platform\")).alias(\"platforms_set\"))\n",
    "    .withColumn(\"group_key\", F.concat_ws(\"-\", F.col(\"platforms_set\")))\n",
    "    .select(\"idtitle\", \"group_key\", \"platforms_set\")\n",
    ")\n",
    "\n",
    "group_dim = (\n",
    "    sig.select(\"group_key\").dropDuplicates()\n",
    "    .withColumn(\n",
    "        \"id_group_platform\",\n",
    "        F.dense_rank().over(Window.orderBy(\"group_key\")).cast(\"int\")\n",
    "    )\n",
    ")\n",
    "\n",
    "map_title_group_platform = (\n",
    "    sig.join(group_dim, \"group_key\", \"inner\")\n",
    "    .select(\"idtitle\", \"id_group_platform\", \"platforms_set\")\n",
    ")\n",
    "\n",
    "bridge_views_platform = (\n",
    "    map_title_group_platform\n",
    "    .select(\"id_group_platform\", F.explode(\"platforms_set\").alias(\"id_platform\"))\n",
    "    .dropDuplicates([\"id_group_platform\", \"id_platform\"])\n",
    ")\n",
    "\n",
    "# 4) Write Gold\n",
    "bridge_views_platform.write.mode(\"overwrite\").parquet(OUT_BRIDGE)\n",
    "map_title_group_platform.select(\"idtitle\", \"id_group_platform\").write.mode(\"overwrite\").parquet(OUT_MAP)\n",
    "\n",
    "spark.read.parquet(OUT_BRIDGE).show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "274f09d1-b7d5-4cee-8181-98846bf13036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|   n|\n",
      "+----+\n",
      "|6846|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "HDFS_NN     = \"hdfs://hdfs-nn:9000\"\n",
    "SILVER_BASE = f\"{HDFS_NN}/demo/silver\"\n",
    "GOLD_BASE   = f\"{HDFS_NN}/demo/golds\"\n",
    "\n",
    "def kt(col_):\n",
    "    return F.lower(F.trim(col_))\n",
    "\n",
    "def has_col(df, c):\n",
    "    return c in df.columns\n",
    "\n",
    "def pick_col(df, candidates, cast=None):\n",
    "    \"\"\"\n",
    "    devolve a primeira coluna existente em df dentro de candidates.\n",
    "    se nenhuma existir, devolve lit(None).\n",
    "    \"\"\"\n",
    "    for c in candidates:\n",
    "        if has_col(df, c):\n",
    "            x = F.col(c)\n",
    "            return x.cast(cast) if cast else x\n",
    "    return F.lit(None).cast(cast) if cast else F.lit(None)\n",
    "\n",
    "def norm_title_expr(c):\n",
    "    \"\"\"\n",
    "    normalização forte para joins por título\n",
    "    \"\"\"\n",
    "    return F.trim(F.regexp_replace(F.lower(F.col(c)), r\"\\s*\\(.*?\\)\\s*\", \"\"))  # remove (2019)\n",
    "\n",
    "\n",
    "def write_gold_table(df, table_name, path):\n",
    "    # evita caches/stale metadata\n",
    "    spark.catalog.clearCache()\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "\n",
    "    # escreve a tabela external diretamente\n",
    "    (df.write\n",
    "       .mode(\"overwrite\")\n",
    "       .format(\"parquet\")\n",
    "       .option(\"path\", path)\n",
    "       .saveAsTable(table_name))\n",
    "\n",
    "# --- usar aqui ---\n",
    "write_gold_table(bridge_launch_genre, \"golds.bridge_launch_genre\", OUT_BRIDGE)\n",
    "\n",
    "spark.sql(\"SELECT COUNT(*) AS n FROM golds.bridge_launch_genre\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4076a206-fc58-4a99-9975-6fb4ee664534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base rows: 9999\n",
      "+--------+---------------------------------+\n",
      "|id_title|title_dim                        |\n",
      "+--------+---------------------------------+\n",
      "|26      |...And Your Name Is Jonah        |\n",
      "|29      |10 Days In Sun City              |\n",
      "|474     |A Walk Among the Tombstones      |\n",
      "|964     |An Evening with Beverly Luff Linn|\n",
      "|1677    |Beyblade Burst Rise              |\n",
      "+--------+---------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "launch_out rows: 9999\n",
      "id_time null: 132\n",
      "+--------+--------+--------------+-------------+--------------+---------+---------+\n",
      "|id_title|id_time |id_group_genre|flag_theaters|flag_streaming|gross    |budget   |\n",
      "+--------+--------+--------------+-------------+--------------+---------+---------+\n",
      "|1       |20180101|null          |0            |1             |null     |null     |\n",
      "|2       |20200101|67            |1            |1             |36733909 |32000000 |\n",
      "|3       |20190101|719           |0            |1             |null     |null     |\n",
      "|4       |20180101|1294          |0            |1             |null     |null     |\n",
      "|5       |20200101|1294          |0            |1             |null     |null     |\n",
      "|6       |20210101|671           |1            |1             |386041607|135000000|\n",
      "|7       |20200101|null          |0            |1             |null     |null     |\n",
      "|8       |20190101|621           |0            |1             |7412216  |1500000  |\n",
      "|9       |20150101|1201          |0            |1             |null     |null     |\n",
      "|10      |20160101|null          |0            |1             |null     |null     |\n",
      "+--------+--------+--------------+-------------+--------------+---------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----+\n",
      "|   n|\n",
      "+----+\n",
      "|9999|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#launch\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "HDFS_NN     = \"hdfs://hdfs-nn:9000\"\n",
    "SILVER_BASE = f\"{HDFS_NN}/demo/silver\"\n",
    "GOLD_BASE   = f\"{HDFS_NN}/demo/golds\"\n",
    "\n",
    "BOX_PATH    = f\"{SILVER_BASE}/boxoffice_info\"\n",
    "RT_PATH     = f\"{SILVER_BASE}/rotten_tomatoes_movies\"\n",
    "TITLES_PATH = f\"{SILVER_BASE}/titles\"\n",
    "MAP_GEN     = f\"{GOLD_BASE}/map_title_group_genre\"\n",
    "\n",
    "OUT_PATH    = f\"{GOLD_BASE}/launch\"\n",
    "DIM_TIME_PATH  = f\"{GOLD_BASE}/dim_time\"\n",
    "\n",
    "def norm_title(c):\n",
    "    x = F.lower(F.trim(c))\n",
    "    x = F.regexp_replace(x, r\"\\s*[\\(\\[]\\s*\\d{4}\\s*[\\)\\]]\\s*\", \"\")\n",
    "    x = F.regexp_replace(x, r\"[^a-z0-9\\s]\", \" \")\n",
    "    x = F.regexp_replace(x, r\"\\s+\", \" \")\n",
    "    return F.trim(x)\n",
    "\n",
    "def write_gold_table(df, table_name, path):\n",
    "    spark.catalog.clearCache()\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "    (df.write\n",
    "       .mode(\"overwrite\")\n",
    "       .format(\"parquet\")\n",
    "       .option(\"path\", path)\n",
    "       .saveAsTable(table_name))\n",
    "    spark.sql(f\"REFRESH TABLE {table_name}\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1) Ler dim_title SEM inventar (já está fix como LONG)\n",
    "# -------------------------------------------------\n",
    "spark.catalog.clearCache()\n",
    "spark.sql(\"REFRESH TABLE golds.dim_title\")\n",
    "dim_title = spark.table(\"golds.dim_title\")\n",
    "\n",
    "# Base: id_title (LONG) + title + key k\n",
    "if \"id_title\" not in dim_title.columns or \"title\" not in dim_title.columns:\n",
    "    raise Exception(\"golds.dim_title tem de ter colunas id_title e title\")\n",
    "\n",
    "base = (dim_title\n",
    "    .select(\n",
    "        F.col(\"id_title\").cast(\"long\").alias(\"id_title\"),\n",
    "        F.col(\"title\").cast(\"string\").alias(\"title_dim\")\n",
    "    )\n",
    "    .filter(F.col(\"title_dim\").isNotNull() & (F.length(F.trim(F.col(\"title_dim\"))) > 0))\n",
    "    .dropDuplicates([\"id_title\"])\n",
    "    .withColumn(\"k\", norm_title(F.col(\"title_dim\")))\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "print(\"base rows:\", base.count())\n",
    "base.select(\"id_title\",\"title_dim\").show(5, truncate=False)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2) Dim_time (metastore -> fallback path)\n",
    "# -------------------------------------------------\n",
    "spark.catalog.clearCache()\n",
    "try:\n",
    "    spark.sql(\"REFRESH TABLE golds.dim_time\")\n",
    "    dim_time = spark.table(\"golds.dim_time\")\n",
    "    _ = dim_time.limit(1).count()\n",
    "except:\n",
    "    dim_time = spark.read.parquet(DIM_TIME_PATH)\n",
    "\n",
    "dim_time = (dim_time\n",
    "    .select(F.col(\"id_time\").cast(\"int\").alias(\"id_time\"),\n",
    "            F.to_date(\"date\").alias(\"date\"))\n",
    "    .dropDuplicates([\"date\"])\n",
    "    .cache()\n",
    ")\n",
    "_ = dim_time.count()\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3) TITLES: release_year + plataformas p/ flag_streaming\n",
    "# -------------------------------------------------\n",
    "titles = spark.read.parquet(TITLES_PATH)\n",
    "title_col = \"title\" if \"title\" in titles.columns else (\"name\" if \"name\" in titles.columns else None)\n",
    "if title_col is None:\n",
    "    raise Exception(\"silver/titles não tem coluna title/name\")\n",
    "\n",
    "t2 = (titles\n",
    "    .select(\n",
    "        norm_title(F.col(title_col)).alias(\"k\"),\n",
    "        (F.col(\"release_year\").cast(\"int\") if \"release_year\" in titles.columns else F.lit(None).cast(\"int\")).alias(\"release_year\"),\n",
    "        (F.coalesce(F.col(\"netflix\").cast(\"int\"), F.lit(0)) if \"netflix\" in titles.columns else F.lit(0)).alias(\"netflix\"),\n",
    "        (F.coalesce(F.col(\"amazon\").cast(\"int\"),  F.lit(0)) if \"amazon\"  in titles.columns else F.lit(0)).alias(\"amazon\"),\n",
    "    )\n",
    "    .dropDuplicates([\"k\"])\n",
    "    .withColumn(\n",
    "        \"launch_date_titles\",\n",
    "        F.when(F.col(\"release_year\").isNotNull(),\n",
    "               F.to_date(F.concat_ws(\"-\", F.col(\"release_year\").cast(\"string\"), F.lit(\"01\"), F.lit(\"01\"))))\n",
    "         .otherwise(F.lit(None).cast(\"date\"))\n",
    "    )\n",
    "    .withColumn(\"has_streaming_platform\", (F.col(\"netflix\") == 1) | (F.col(\"amazon\") == 1))\n",
    "    .select(\"k\",\"launch_date_titles\",\"has_streaming_platform\")\n",
    ")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4) BOXOFFICE\n",
    "# -------------------------------------------------\n",
    "box = spark.read.parquet(BOX_PATH)\n",
    "box_title_col = \"title\" if \"title\" in box.columns else (\"movie_title\" if \"movie_title\" in box.columns else None)\n",
    "if box_title_col is None:\n",
    "    raise Exception(\"boxoffice_info não tem title/movie_title\")\n",
    "\n",
    "box2 = (box\n",
    "    .select(\n",
    "        norm_title(F.col(box_title_col)).alias(\"k\"),\n",
    "        (F.to_date(F.col(\"released\").cast(\"string\")) if \"released\" in box.columns else F.lit(None).cast(\"date\")).alias(\"launch_date_box\"),\n",
    "        (F.regexp_replace(F.col(\"gross\").cast(\"string\"),  r\"[^0-9]\", \"\").cast(\"int\") if \"gross\" in box.columns else F.lit(None).cast(\"int\")).alias(\"gross\"),\n",
    "        (F.regexp_replace(F.col(\"budget\").cast(\"string\"), r\"[^0-9]\", \"\").cast(\"int\") if \"budget\" in box.columns else F.lit(None).cast(\"int\")).alias(\"budget\"),\n",
    "    )\n",
    "    .groupBy(\"k\")\n",
    "    .agg(F.max(\"launch_date_box\").alias(\"launch_date_box\"),\n",
    "         F.max(\"gross\").alias(\"gross\"),\n",
    "         F.max(\"budget\").alias(\"budget\"))\n",
    ")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5) RT\n",
    "# -------------------------------------------------\n",
    "rt = spark.read.parquet(RT_PATH)\n",
    "rt_title_col = \"movie_title\" if \"movie_title\" in rt.columns else (\"title\" if \"title\" in rt.columns else None)\n",
    "if rt_title_col is None:\n",
    "    raise Exception(\"rotten_tomatoes_movies não tem title/movie_title\")\n",
    "\n",
    "rt2 = (rt\n",
    "    .select(\n",
    "        norm_title(F.col(rt_title_col)).alias(\"k\"),\n",
    "        (F.to_date(F.col(\"in_theaters_date\")) if \"in_theaters_date\" in rt.columns else F.lit(None).cast(\"date\")).alias(\"in_theaters_date\"),\n",
    "        (F.to_date(F.col(\"on_streaming_date\")) if \"on_streaming_date\" in rt.columns else F.lit(None).cast(\"date\")).alias(\"on_streaming_date\"),\n",
    "    )\n",
    "    .groupBy(\"k\")\n",
    "    .agg(F.max(\"in_theaters_date\").alias(\"in_theaters_date\"),\n",
    "         F.max(\"on_streaming_date\").alias(\"on_streaming_date\"))\n",
    ")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 6) JOINs + launch_date_final + id_time\n",
    "# -------------------------------------------------\n",
    "j = (base\n",
    "    .join(t2, \"k\", \"left\")\n",
    "    .join(box2, \"k\", \"left\")\n",
    "    .join(rt2, \"k\", \"left\")\n",
    ")\n",
    "\n",
    "j = j.withColumn(\n",
    "    \"launch_date_final\",\n",
    "    F.coalesce(\"launch_date_titles\", \"launch_date_box\", \"in_theaters_date\", \"on_streaming_date\")\n",
    ")\n",
    "\n",
    "j = j.join(dim_time, dim_time.date == F.col(\"launch_date_final\"), \"left\")\n",
    "\n",
    "# genre map (se existir)\n",
    "try:\n",
    "    map_genre = (spark.read.parquet(MAP_GEN)\n",
    "        .select(F.col(\"id_title\").cast(\"long\").alias(\"id_title\"),\n",
    "                F.col(\"id_group_genre\").cast(\"int\").alias(\"id_group_genre\"))\n",
    "        .dropDuplicates([\"id_title\"])\n",
    "    )\n",
    "    j = j.join(map_genre, \"id_title\", \"left\")\n",
    "except Exception as e:\n",
    "    print(\"Sem MAP_GEN (map_title_group_genre):\", e)\n",
    "    j = j.withColumn(\"id_group_genre\", F.lit(None).cast(\"int\"))\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 7) FLAGS 0/1 (INT) \n",
    "# -------------------------------------------------\n",
    "j = (j\n",
    "    .withColumn(\"flag_theaters\",\n",
    "        F.when(F.col(\"in_theaters_date\").isNotNull(), 1).otherwise(0).cast(\"int\")\n",
    "    )\n",
    "    .withColumn(\"flag_streaming\",\n",
    "        F.when(\n",
    "            (F.col(\"on_streaming_date\").isNotNull()) | (F.coalesce(F.col(\"has_streaming_platform\"), F.lit(False))),\n",
    "            1\n",
    "        ).otherwise(0).cast(\"int\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 8) OUTPUT\n",
    "# -------------------------------------------------\n",
    "launch_out = (j.select(\n",
    "        F.col(\"id_title\").cast(\"long\").alias(\"id_title\"),\n",
    "        F.col(\"id_time\").cast(\"int\").alias(\"id_time\"),\n",
    "        F.col(\"id_group_genre\").cast(\"int\").alias(\"id_group_genre\"),\n",
    "        F.col(\"flag_theaters\").cast(\"int\").alias(\"flag_theaters\"),\n",
    "        F.col(\"flag_streaming\").cast(\"int\").alias(\"flag_streaming\"),\n",
    "        F.col(\"gross\").cast(\"int\").alias(\"gross\"),\n",
    "        F.col(\"budget\").cast(\"int\").alias(\"budget\")\n",
    "    ).dropDuplicates([\"id_title\"])\n",
    ")\n",
    "\n",
    "print(\"launch_out rows:\", launch_out.count())\n",
    "print(\"id_time null:\", launch_out.filter(F.col(\"id_time\").isNull()).count())\n",
    "launch_out.show(10, truncate=False)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 9) Escrever tabela\n",
    "# -------------------------------------------------\n",
    "write_gold_table(launch_out, \"golds.launch\", OUT_PATH)\n",
    "spark.sql(\"SELECT COUNT(*) AS n FROM golds.launch\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad64e04f-d53a-40c4-8dcb-6e1ad2a452ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awards_fact rows: 6178\n",
      "null id_title: 5346\n",
      "null id_person: 2506\n",
      "null id_award: 0\n",
      "+--------+--------+--------+---------+-------------+---------+----------------+---------------+\n",
      "|id_award|id_title|id_time |id_person|is_nomination|is_winner|is_oscar_nominee|is_oscar_winner|\n",
      "+--------+--------+--------+---------+-------------+---------+----------------+---------------+\n",
      "|44      |null    |20220101|null     |1            |0        |1               |0              |\n",
      "|33      |null    |19640101|null     |1            |0        |1               |0              |\n",
      "|41      |null    |19370101|39107    |1            |1        |1               |1              |\n",
      "|55      |null    |20180101|null     |1            |0        |1               |0              |\n",
      "|23      |null    |19660101|null     |1            |0        |1               |0              |\n",
      "|16      |null    |19540101|null     |1            |1        |1               |1              |\n",
      "|2       |null    |19850101|null     |1            |1        |1               |1              |\n",
      "|6       |null    |19700101|null     |1            |0        |1               |0              |\n",
      "|45      |null    |19370101|null     |1            |0        |1               |0              |\n",
      "|3       |null    |19980101|40434    |1            |1        |1               |1              |\n",
      "|24      |null    |19390101|null     |1            |0        |1               |0              |\n",
      "|55      |null    |19900101|132558   |1            |1        |1               |1              |\n",
      "|1       |null    |20150101|11308    |1            |0        |1               |0              |\n",
      "|6       |null    |20120101|null     |1            |0        |1               |0              |\n",
      "|56      |null    |20200101|84837    |1            |0        |1               |0              |\n",
      "|1       |null    |19650101|87400    |1            |0        |1               |0              |\n",
      "|14      |null    |19660101|null     |1            |0        |1               |0              |\n",
      "|11      |null    |19550101|null     |1            |1        |1               |1              |\n",
      "|34      |null    |19370101|null     |1            |0        |1               |0              |\n",
      "|55      |null    |19860101|144810   |1            |1        |1               |1              |\n",
      "+--------+--------+--------+---------+-------------+---------+----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----+\n",
      "|   n|\n",
      "+----+\n",
      "|6178|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#awards\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "HDFS_NN     = \"hdfs://hdfs-nn:9000\"\n",
    "SILVER_BASE = f\"{HDFS_NN}/demo/silver\"\n",
    "GOLD_BASE   = f\"{HDFS_NN}/demo/golds\"\n",
    "\n",
    "OSC_PATH = f\"{SILVER_BASE}/the_oscar_award\"\n",
    "OUT_PATH = f\"{GOLD_BASE}/awards\"\n",
    "\n",
    "def kt(c):\n",
    "    # normalização simples (bate com a tua limpeza do silver)\n",
    "    return F.lower(F.trim(c))\n",
    "\n",
    "def write_gold_table(df, table_name, path):\n",
    "    spark.catalog.clearCache()\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "    (df.write\n",
    "       .mode(\"overwrite\")\n",
    "       .format(\"parquet\")\n",
    "       .option(\"path\", path)\n",
    "       .saveAsTable(table_name))\n",
    "    spark.sql(f\"REFRESH TABLE {table_name}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Ler Oscars (silver)\n",
    "# -----------------------------\n",
    "osc = spark.read.parquet(OSC_PATH)\n",
    "\n",
    "# garantir colunas necessárias\n",
    "need = [\"year_ceremony\", \"canon_category\", \"name\", \"film\", \"winner\"]\n",
    "missing = [c for c in need if c not in osc.columns]\n",
    "if missing:\n",
    "    raise Exception(f\"Faltam colunas no silver/the_oscar_award: {missing}. Tenho: {osc.columns}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Ler dims (golds) — IMPORTANTE: id_title como LONG\n",
    "# -----------------------------\n",
    "dim_title = (spark.table(\"golds.dim_title\")\n",
    "    .select(\n",
    "        F.col(\"id_title\").cast(\"long\").alias(\"id_title\"),\n",
    "        kt(F.col(\"title\")).alias(\"k_title\")\n",
    "    )\n",
    "    .dropDuplicates([\"k_title\"])\n",
    ")\n",
    "\n",
    "dim_person = (spark.table(\"golds.dim_person\")\n",
    "    .select(\n",
    "        F.col(\"id_person\").cast(\"int\").alias(\"id_person\"),\n",
    "        kt(F.col(\"name\")).alias(\"k_name\")\n",
    "    )\n",
    "    .dropDuplicates([\"k_name\"])\n",
    ")\n",
    "\n",
    "dim_category = (spark.table(\"golds.dim_category\")\n",
    "    .select(\n",
    "        F.col(\"id_award\").cast(\"int\").alias(\"id_award\"),\n",
    "        kt(F.col(\"canonical_category\")).alias(\"k_cat\")\n",
    "    )\n",
    "    .dropDuplicates([\"k_cat\"])\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Preparar osc: keys + id_time\n",
    "# id_time = YYYY0101 (int)  -> se quiseres depois eu faço join com dim_time real\n",
    "# -----------------------------\n",
    "aw = (osc\n",
    "    .withColumn(\"k_title\", kt(F.col(\"film\")))\n",
    "    .withColumn(\"k_name\",  kt(F.col(\"name\")))\n",
    "    .withColumn(\"k_cat\",   kt(F.col(\"canon_category\")))\n",
    "    .withColumn(\"id_time\", (F.col(\"year_ceremony\").cast(\"int\") * F.lit(10000) + F.lit(101)).cast(\"int\"))\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 4) winner -> 0/1 int (robusto)\n",
    "# -----------------------------\n",
    "winner01 = (\n",
    "    F.when(F.col(\"winner\") == True, 1)\n",
    "     .when(F.lower(F.col(\"winner\").cast(\"string\")) == \"true\", 1)\n",
    "     .when(F.col(\"winner\").cast(\"int\") == 1, 1)\n",
    "     .otherwise(0)\n",
    "     .cast(\"int\")\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Construir fact awards (tudo 0/1 em INT)\n",
    "# -----------------------------\n",
    "awards_fact = (\n",
    "    aw.join(dim_title,    \"k_title\", \"left\")\n",
    "      .join(dim_person,   \"k_name\",  \"left\")\n",
    "      .join(dim_category, \"k_cat\",   \"left\")\n",
    "      .withColumn(\"is_nomination\",    F.lit(1).cast(\"int\"))\n",
    "      .withColumn(\"is_winner\",        winner01)\n",
    "      .withColumn(\"is_oscar_nominee\", F.lit(1).cast(\"int\"))\n",
    "      .withColumn(\"is_oscar_winner\",  winner01)\n",
    "      .select(\n",
    "          F.col(\"id_award\").cast(\"int\").alias(\"id_award\"),\n",
    "          F.col(\"id_title\").cast(\"long\").alias(\"id_title\"),   # LONG ✅\n",
    "          F.col(\"id_time\").cast(\"int\").alias(\"id_time\"),\n",
    "          F.col(\"id_person\").cast(\"int\").alias(\"id_person\"),\n",
    "          F.col(\"is_nomination\").cast(\"int\").alias(\"is_nomination\"),\n",
    "          F.col(\"is_winner\").cast(\"int\").alias(\"is_winner\"),\n",
    "          F.col(\"is_oscar_nominee\").cast(\"int\").alias(\"is_oscar_nominee\"),\n",
    "          F.col(\"is_oscar_winner\").cast(\"int\").alias(\"is_oscar_winner\"),\n",
    "      )\n",
    "      .dropDuplicates([\"id_award\", \"id_title\", \"id_time\", \"id_person\"])\n",
    ")\n",
    "\n",
    "print(\"awards_fact rows:\", awards_fact.count())\n",
    "print(\"null id_title:\", awards_fact.filter(F.col(\"id_title\").isNull()).count())\n",
    "print(\"null id_person:\", awards_fact.filter(F.col(\"id_person\").isNull()).count())\n",
    "print(\"null id_award:\", awards_fact.filter(F.col(\"id_award\").isNull()).count())\n",
    "awards_fact.show(20, truncate=False)\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Escrever tabela\n",
    "# -----------------------------\n",
    "write_gold_table(awards_fact, \"golds.awards\", OUT_PATH)\n",
    "spark.sql(\"SELECT COUNT(*) AS n FROM golds.awards\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e1b8643-730c-4318-a97a-97da7be19dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 179\u001b[0m\n\u001b[1;32m    146\u001b[0m tmdb_df \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    147\u001b[0m     titles\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;241m.\u001b[39mselect(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;241m.\u001b[39mselect(cols)\n\u001b[1;32m    163\u001b[0m )\n\u001b[1;32m    165\u001b[0m rating_union \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    166\u001b[0m     ensure_cols(rating_rot)\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;241m.\u001b[39munionByName(ensure_cols(imdb_df))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;241m.\u001b[39mrepartition(\u001b[38;5;241m4\u001b[39m)   \u001b[38;5;66;03m# ajuda a controlar o número de ficheiros\u001b[39;00m\n\u001b[1;32m    171\u001b[0m )\n\u001b[1;32m    173\u001b[0m (\n\u001b[1;32m    174\u001b[0m     \u001b[43mrating_union\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRATING_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m--> 179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgolds.rating\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:1521\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m-> 1521\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#rating\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "HDFS_NN = \"hdfs://hdfs-nn:9000\"\n",
    "SILVER  = f\"{HDFS_NN}/demo/silver\"\n",
    "GOLD    = f\"{HDFS_NN}/demo/golds\"\n",
    "\n",
    "RT_SILVER   = f\"{SILVER}/rotten_tomatoes_movies\"\n",
    "TITLES_PATH = f\"{SILVER}/titles\"\n",
    "DRS_PATH    = f\"{GOLD}/dim_rating_status\"\n",
    "RATING_PATH = f\"{GOLD}/rating\"\n",
    "\n",
    "cols = [\n",
    "    \"id_title\",\n",
    "    \"id_rating_source_status\",\n",
    "    \"id_time\",\n",
    "    \"critic_score\",\n",
    "    \"audience_score\",\n",
    "    \"critic_reviews\",\n",
    "    \"audience_reviews\",\n",
    "    \"rt_critic_audience\",\n",
    "    \"popularity_index\"\n",
    "]\n",
    "\n",
    "def ensure_cols(df: DataFrame) -> DataFrame:\n",
    "    for c in cols:\n",
    "        if c not in df.columns:\n",
    "            df = df.withColumn(c, F.lit(None))\n",
    "    return df.select(cols)\n",
    "\n",
    "def norm_title(c):\n",
    "    x = F.lower(F.trim(c))\n",
    "    x = F.regexp_replace(x, r\"\\s*\\(.*?\\)\\s*\", \" \")\n",
    "    x = F.regexp_replace(x, r\"[^\\p{L}\\p{N}\\s]\", \" \")\n",
    "    x = F.regexp_replace(x, r\"\\s+\", \" \")\n",
    "    return F.trim(x)\n",
    "\n",
    "def norm_key(c):\n",
    "    x = F.lower(F.trim(c))\n",
    "    x = F.regexp_replace(x, r\"[^\\p{L}\\p{N}\\s]\", \" \")\n",
    "    x = F.regexp_replace(x, r\"\\s+\", \" \")\n",
    "    return F.trim(x)\n",
    "\n",
    "# 1) Rotten completo\n",
    "rt = spark.read.parquet(RT_SILVER)\n",
    "\n",
    "dim_title_keys = (\n",
    "    spark.read.parquet(f\"{GOLD}/dim_title\")\n",
    "    .select(\n",
    "        F.col(\"id_title\").cast(\"bigint\").alias(\"id_title\"),\n",
    "        norm_title(F.col(\"title\")).alias(\"k_title\")\n",
    "    )\n",
    "    .dropDuplicates([\"k_title\"])\n",
    ")\n",
    "\n",
    "dim_rs_keys = (\n",
    "    spark.read.parquet(DRS_PATH)\n",
    "    .select(\n",
    "        F.col(\"id_rating_source_status\").cast(\"bigint\").alias(\"id_rating_source_status\"),\n",
    "        norm_key(F.col(\"source\")).alias(\"k_source\"),\n",
    "        norm_key(F.col(\"tomatometer_status\")).alias(\"k_status\")\n",
    "    )\n",
    "    .dropDuplicates([\"k_source\", \"k_status\"])\n",
    ")\n",
    "\n",
    "rt_pre = (\n",
    "    rt\n",
    "    .withColumn(\"in_theaters_date\", F.to_date(\"in_theaters_date\"))\n",
    "    .withColumn(\"on_streaming_date\", F.to_date(\"on_streaming_date\"))\n",
    "    .withColumn(\"rating_date\", F.coalesce(F.col(\"in_theaters_date\"), F.col(\"on_streaming_date\")))\n",
    "    .withColumn(\n",
    "        \"id_time\",\n",
    "        F.when(F.col(\"rating_date\").isNotNull(), F.date_format(\"rating_date\", \"yyyyMMdd\").cast(\"int\"))\n",
    "         .otherwise(F.lit(None).cast(\"int\"))\n",
    "    )\n",
    "    .withColumn(\"k_title\", norm_title(F.col(\"movie_title\")))\n",
    "    .withColumn(\"k_source\", norm_key(F.lit(\"rotten_tomatoes\")))\n",
    "    .withColumn(\"k_status\", norm_key(F.col(\"tomatometer_status\")))\n",
    ")\n",
    "\n",
    "rating_rot = (\n",
    "    rt_pre\n",
    "    .join(dim_title_keys, \"k_title\", \"left\")\n",
    "    .join(dim_rs_keys, [\"k_source\", \"k_status\"], \"left\")\n",
    "    .withColumn(\"critic_score\", F.col(\"tomatometer_rating\").cast(\"double\"))\n",
    "    .withColumn(\"audience_score\", F.col(\"audience_rating\").cast(\"double\"))\n",
    "    .withColumn(\"critic_reviews\", F.col(\"tomatometer_count\").cast(\"int\"))\n",
    "    .withColumn(\"audience_reviews\", F.col(\"audience_count\").cast(\"int\"))\n",
    "    .withColumn(\n",
    "        \"rt_critic_audience\",\n",
    "        (F.col(\"critic_score\") - F.col(\"audience_score\")).cast(\"double\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"popularity_index\",\n",
    "        (\n",
    "            F.coalesce(F.col(\"critic_reviews\").cast(\"double\"), F.lit(0.0)) +\n",
    "            F.coalesce(F.col(\"audience_reviews\").cast(\"double\"), F.lit(0.0))\n",
    "        ).cast(\"double\")\n",
    "    )\n",
    "    .select(\n",
    "        F.col(\"id_title\").cast(\"bigint\").alias(\"id_title\"),\n",
    "        F.col(\"id_rating_source_status\").cast(\"bigint\").alias(\"id_rating_source_status\"),\n",
    "        F.col(\"id_time\").cast(\"int\").alias(\"id_time\"),\n",
    "        \"critic_score\",\n",
    "        \"audience_score\",\n",
    "        \"critic_reviews\",\n",
    "        \"audience_reviews\",\n",
    "        \"rt_critic_audience\",\n",
    "        \"popularity_index\"\n",
    "    )\n",
    "    .dropDuplicates([\"id_title\", \"id_rating_source_status\", \"id_time\"])\n",
    ")\n",
    "\n",
    "# 2) IMDB/TMDB completos\n",
    "titles = spark.read.parquet(TITLES_PATH)\n",
    "drs    = spark.read.parquet(DRS_PATH)\n",
    "\n",
    "imdb_key = drs.filter((F.col(\"source\") == \"imdb\") & (F.col(\"tomatometer_status\") == \"score\")) \\\n",
    "              .select(\"id_rating_source_status\").first()\n",
    "tmdb_key = drs.filter((F.col(\"source\") == \"tmdb\") & (F.col(\"tomatometer_status\") == \"score\")) \\\n",
    "              .select(\"id_rating_source_status\").first()\n",
    "\n",
    "id_imdb = imdb_key[\"id_rating_source_status\"]\n",
    "id_tmdb = tmdb_key[\"id_rating_source_status\"]\n",
    "\n",
    "imdb_df = (\n",
    "    titles\n",
    "    .select(\n",
    "        F.col(\"id_title\").cast(\"bigint\").alias(\"id_title\"),\n",
    "        F.col(\"imdb_score\").cast(\"double\").alias(\"score\"),\n",
    "        F.col(\"imdb_votes\").cast(\"int\").alias(\"votes\")\n",
    "    )\n",
    "    .filter(F.col(\"score\").isNotNull())\n",
    "    .withColumn(\"id_rating_source_status\", F.lit(id_imdb).cast(\"bigint\"))\n",
    "    .withColumn(\"id_time\", F.lit(None).cast(\"int\"))\n",
    "    .withColumn(\"critic_score\", F.lit(None).cast(\"double\"))\n",
    "    .withColumn(\"audience_score\", F.col(\"score\"))\n",
    "    .withColumn(\"critic_reviews\", F.lit(None).cast(\"int\"))\n",
    "    .withColumn(\"audience_reviews\", F.col(\"votes\"))\n",
    "    .withColumn(\"rt_critic_audience\", F.lit(None).cast(\"double\"))\n",
    "    .withColumn(\"popularity_index\", F.col(\"votes\").cast(\"double\"))\n",
    "    .select(cols)\n",
    ")\n",
    "\n",
    "tmdb_df = (\n",
    "    titles\n",
    "    .select(\n",
    "        F.col(\"id_title\").cast(\"bigint\").alias(\"id_title\"),\n",
    "        F.col(\"tmdb_score\").cast(\"double\").alias(\"score\"),\n",
    "        F.col(\"tmdb_popularity\").cast(\"double\").alias(\"popularity\")\n",
    "    )\n",
    "    .filter(F.col(\"score\").isNotNull())\n",
    "    .withColumn(\"id_rating_source_status\", F.lit(id_tmdb).cast(\"bigint\"))\n",
    "    .withColumn(\"id_time\", F.lit(None).cast(\"int\"))\n",
    "    .withColumn(\"critic_score\", F.lit(None).cast(\"double\"))\n",
    "    .withColumn(\"audience_score\", F.col(\"score\"))\n",
    "    .withColumn(\"critic_reviews\", F.lit(None).cast(\"int\"))\n",
    "    .withColumn(\"audience_reviews\", F.lit(None).cast(\"int\"))\n",
    "    .withColumn(\"rt_critic_audience\", F.lit(None).cast(\"double\"))\n",
    "    .withColumn(\"popularity_index\", F.col(\"popularity\"))\n",
    "    .select(cols)\n",
    ")\n",
    "\n",
    "rating_union = (\n",
    "    ensure_cols(rating_rot)\n",
    "    .unionByName(ensure_cols(imdb_df))\n",
    "    .unionByName(ensure_cols(tmdb_df))\n",
    "    .dropDuplicates([\"id_title\", \"id_rating_source_status\", \"id_time\"])\n",
    "    .repartition(4)   # ajuda a controlar o número de ficheiros\n",
    ")\n",
    "\n",
    "(\n",
    "    rating_union\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"parquet\")\n",
    "    .option(\"path\", RATING_PATH)\n",
    "    .saveAsTable(\"golds.rating\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22a651f-addf-44d5-abb8-a4a6d252901f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#views\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "HDFS_NN     = \"hdfs://hdfs-nn:9000\"\n",
    "SILVER_BASE = f\"{HDFS_NN}/demo/silver\"\n",
    "GOLD_BASE   = f\"{HDFS_NN}/demo/golds\"\n",
    "\n",
    "PIRACY_PATH = f\"{SILVER_BASE}/dataset_piracy\"\n",
    "DIM_TITLE_PATH = f\"{GOLD_BASE}/dim_title\"\n",
    "MAP_TITLE_GROUP_PLATFORM_PATH = f\"{GOLD_BASE}/map_title_group_platform\"\n",
    "\n",
    "OUT_VIEWS = f\"{GOLD_BASE}/views\"\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def kt(c):\n",
    "    return F.lower(F.trim(c))\n",
    "\n",
    "def pick_col(df, candidates):\n",
    "    cols = set(df.columns)\n",
    "    for c in candidates:\n",
    "        if c in cols:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def to_date_flex(col):\n",
    "    # tenta vários formatos comuns\n",
    "    return F.coalesce(\n",
    "        F.to_date(col),\n",
    "        F.to_date(col, \"yyyy-MM-dd\"),\n",
    "        F.to_date(col, \"yyyy/MM/dd\"),\n",
    "        F.to_date(col, \"dd-MM-yyyy\"),\n",
    "        F.to_date(col, \"dd/MM/yyyy\"),\n",
    "        F.to_date(col, \"MM/dd/yyyy\"),\n",
    "        F.to_date(col, \"MM-dd-yyyy\"),\n",
    "    )\n",
    "\n",
    "# ---------- read sources ----------\n",
    "pir = spark.read.parquet(PIRACY_PATH)\n",
    "\n",
    "dim_title = spark.read.parquet(DIM_TITLE_PATH)\n",
    "\n",
    "# id_title pode ser id_title ou idtitle, normalizamos\n",
    "id_title_col = \"id_title\" if \"id_title\" in dim_title.columns else (\n",
    "    \"idtitle\" if \"idtitle\" in dim_title.columns else None\n",
    ")\n",
    "if id_title_col is None:\n",
    "    raise Exception(\"Não encontrei coluna id_title nem idtitle em golds.dim_title\")\n",
    "\n",
    "dim_title_lu = (\n",
    "    dim_title\n",
    "    .select(\n",
    "        F.col(id_title_col).cast(\"bigint\").alias(\"id_title\"),\n",
    "        kt(F.col(\"title\")).alias(\"k_title\")\n",
    "    )\n",
    "    .dropDuplicates([\"k_title\"])\n",
    ")\n",
    "\n",
    "map_gp = spark.read.parquet(MAP_TITLE_GROUP_PLATFORM_PATH)\n",
    "\n",
    "map_id_col = \"id_title\" if \"id_title\" in map_gp.columns else (\n",
    "    \"idtitle\" if \"idtitle\" in map_gp.columns else None\n",
    ")\n",
    "if map_id_col is None:\n",
    "    raise Exception(\"Não encontrei coluna id_title nem idtitle em golds.map_title_group_platform\")\n",
    "\n",
    "map_platform = (\n",
    "    map_gp\n",
    "    .select(\n",
    "        F.col(map_id_col).cast(\"bigint\").alias(\"id_title\"),\n",
    "        F.col(\"id_group_platform\").cast(\"int\").alias(\"id_group_platform\")\n",
    "    )\n",
    "    .dropDuplicates([\"id_title\"])\n",
    ")\n",
    "\n",
    "# ---------- detect columns in piracy ----------\n",
    "title_col   = pick_col(pir, [\"title\", \"movie_title\", \"movie\", \"film\", \"name\", \"movie_name\"])\n",
    "views_col   = pick_col(pir, [\"views\", \"view_count\", \"views_pirate\", \"views_price\"])\n",
    "down_col    = pick_col(pir, [\"downloads\", \"download_count\", \"downloads_pirate\", \"downloads_price\"])\n",
    "posted_col  = pick_col(pir, [\"posted_date\", \"post_date\", \"date_posted\", \"leak_date\"])\n",
    "release_col = pick_col(pir, [\"release_date\", \"released_date\", \"date_release\", \"premiere_date\"])\n",
    "\n",
    "if title_col is None or posted_col is None:\n",
    "    raise Exception(\n",
    "        f\"Não encontrei colunas essenciais no piracy: title_col={title_col}, posted_col={posted_col}\"\n",
    "    )\n",
    "\n",
    "# ---------- staging ----------\n",
    "pir2 = (\n",
    "    pir\n",
    "    .withColumn(\"title_raw\", F.col(title_col).cast(\"string\"))\n",
    "    .withColumn(\"k_title\", kt(F.col(\"title_raw\")))\n",
    "    .withColumn(\"posted_date\", to_date_flex(F.col(posted_col).cast(\"string\")))\n",
    "    .withColumn(\n",
    "        \"release_date\",\n",
    "        to_date_flex(F.col(release_col).cast(\"string\")) if release_col else F.lit(None).cast(\"date\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"views_pirate\",\n",
    "        F.col(views_col).cast(\"long\") if views_col else F.lit(None).cast(\"long\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"downloads_pirate\",\n",
    "        F.col(down_col).cast(\"long\") if down_col else F.lit(None).cast(\"long\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# id_time = yyyyMMdd do posted_date\n",
    "pir2 = pir2.withColumn(\n",
    "    \"id_time\",\n",
    "    F.when(\n",
    "        F.col(\"posted_date\").isNotNull(),\n",
    "        F.date_format(F.col(\"posted_date\"), \"yyyyMMdd\").cast(\"int\")\n",
    "    ).otherwise(F.lit(None).cast(\"int\"))\n",
    ")\n",
    "\n",
    "# join para id_title\n",
    "views_staging = (\n",
    "    pir2\n",
    "    .join(dim_title_lu, \"k_title\", \"left\")\n",
    ")\n",
    "\n",
    "# join para id_group_platform (SEM forçar 0; deixamos NULL quando não há mapeamento)\n",
    "views_staging = (\n",
    "    views_staging\n",
    "    .join(map_platform, \"id_title\", \"left\")\n",
    "    .withColumn(\"id_group_platform\", F.col(\"id_group_platform\").cast(\"int\"))\n",
    ")\n",
    "\n",
    "# days_since_release = posted_date - release_date\n",
    "views_staging = views_staging.withColumn(\n",
    "    \"days_since_release\",\n",
    "    F.when(\n",
    "        F.col(\"posted_date\").isNotNull() & F.col(\"release_date\").isNotNull(),\n",
    "        F.datediff(F.col(\"posted_date\"), F.col(\"release_date\"))\n",
    "    ).otherwise(F.lit(None).cast(\"int\"))\n",
    ")\n",
    "\n",
    "# first_leak_flag = 1 no primeiro posted_date por id_title\n",
    "w = Window.partitionBy(\"id_title\").orderBy(F.col(\"posted_date\").asc_nulls_last())\n",
    "views_staging = (\n",
    "    views_staging\n",
    "    .withColumn(\"rn_first\", F.row_number().over(w))\n",
    "    .withColumn(\n",
    "        \"first_leak_flag\",\n",
    "        F.when(F.col(\"rn_first\") == 1, F.lit(True)).otherwise(F.lit(False))\n",
    "    )\n",
    "    .drop(\"rn_first\")\n",
    ")\n",
    "\n",
    "# ---------- AGREGAR para evitar duplicados (mesma PK) ----------\n",
    "# PK: id_title, id_time, id_group_platform\n",
    "views_fact = (\n",
    "    views_staging\n",
    "    .groupBy(\"id_title\", \"id_time\", \"id_group_platform\")\n",
    "    .agg(\n",
    "        F.max(\"views_pirate\").alias(\"views_pirate\"),\n",
    "        F.max(\"downloads_pirate\").alias(\"downloads_pirate\"),\n",
    "        F.max(F.col(\"first_leak_flag\").cast(\"int\")).cast(\"int\").alias(\"first_leak_flag\"),\n",
    "        F.min(\"days_since_release\").alias(\"days_since_release\")\n",
    "    )\n",
    "    .select(\n",
    "        F.col(\"id_title\").cast(\"bigint\").alias(\"id_title\"),\n",
    "        F.col(\"id_time\").cast(\"int\").alias(\"id_time\"),\n",
    "        F.col(\"id_group_platform\").cast(\"int\").alias(\"id_group_platform\"),\n",
    "        F.col(\"views_pirate\").cast(\"long\").alias(\"views_pirate\"),\n",
    "        F.col(\"downloads_pirate\").cast(\"long\").alias(\"downloads_pirate\"),\n",
    "        F.col(\"first_leak_flag\").cast(\"int\").alias(\"first_leak_flag\"),\n",
    "        F.col(\"days_since_release\").cast(\"int\").alias(\"days_since_release\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# ---------- write ----------\n",
    "(\n",
    "    views_fact.write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"parquet\")\n",
    "    .option(\"path\", OUT_VIEWS)\n",
    "    .saveAsTable(\"golds.views\")\n",
    ")\n",
    "\n",
    "# ---------- quick checks ----------\n",
    "spark.sql(\"SELECT COUNT(*) AS n FROM golds.views\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "  SUM(CASE WHEN id_title IS NULL THEN 1 ELSE 0 END) AS null_id_title,\n",
    "  SUM(CASE WHEN id_time IS NULL THEN 1 ELSE 0 END) AS null_id_time,\n",
    "  SUM(CASE WHEN id_group_platform IS NULL THEN 1 ELSE 0 END) AS null_id_group_platform\n",
    "FROM golds.views\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT id_group_platform, COUNT(*) AS n\n",
    "FROM golds.views\n",
    "GROUP BY id_group_platform\n",
    "ORDER BY id_group_platform\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM golds.views\n",
    "LIMIT 200\n",
    "\"\"\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2b33982-ea2a-43c2-85e3-af8c5bb9966d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mtrim(x)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# --------- ler actor_films ----------\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m actor_films \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mparquet(ACTOR_FILMS_PATH)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# garantir colunas essenciais\u001b[39;00m\n\u001b[1;32m     28\u001b[0m need \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mActor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFilm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "#participation\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "HDFS_NN = \"hdfs://hdfs-nn:9000\"\n",
    "SILVER  = f\"{HDFS_NN}/demo/silver\"\n",
    "GOLD    = f\"{HDFS_NN}/demo/golds\"\n",
    "\n",
    "ACTOR_FILMS_PATH = f\"{SILVER}/actorfilms\"\n",
    "OUT_PATH         = f\"{GOLD}/participation\"\n",
    "\n",
    "# --------- helpers ----------\n",
    "def norm_name(c):\n",
    "    x = F.lower(F.trim(c))\n",
    "    x = F.regexp_replace(x, r\"\\s+\", \" \")\n",
    "    return F.trim(x)\n",
    "\n",
    "def norm_title(c):\n",
    "    x = F.lower(F.trim(c))\n",
    "    x = F.regexp_replace(x, r\"\\s*\\(.*?\\)\\s*\", \" \")\n",
    "    x = F.regexp_replace(x, r\"[^\\p{L}\\p{N}\\s]\", \" \")\n",
    "    x = F.regexp_replace(x, r\"\\s+\", \" \")\n",
    "    return F.trim(x)\n",
    "\n",
    "# --------- ler actor_films ----------\n",
    "actor_films = spark.read.parquet(ACTOR_FILMS_PATH)\n",
    "\n",
    "# garantir colunas essenciais\n",
    "need = [\"Actor\", \"Film\", \"Year\"]\n",
    "missing = [c for c in need if c not in actor_films.columns]\n",
    "if missing:\n",
    "    raise Exception(f\"Faltam colunas em silver/actor_films: {missing}. Tenho: {actor_films.columns}\")\n",
    "\n",
    "af = (\n",
    "    actor_films\n",
    "    .select(\n",
    "        norm_name(F.col(\"Actor\")).alias(\"k_name\"),\n",
    "        norm_title(F.col(\"Film\")).alias(\"k_title\"),\n",
    "        F.col(\"Year\").cast(\"int\").alias(\"year\"),\n",
    "    )\n",
    "    .filter(F.col(\"k_name\").isNotNull() & F.col(\"k_title\").isNotNull())\n",
    "    .dropDuplicates()\n",
    ")\n",
    "\n",
    "# --------- dims ----------\n",
    "dim_person = (\n",
    "    spark.table(\"golds.dim_person\")\n",
    "    .select(\n",
    "        F.col(\"id_person\").cast(\"int\").alias(\"id_person\"),\n",
    "        norm_name(F.col(\"name\")).alias(\"k_name\"),\n",
    "        F.col(\"person_type\")\n",
    "    )\n",
    "    .dropDuplicates([\"k_name\", \"person_type\"])\n",
    ")\n",
    "\n",
    "dim_title = (\n",
    "    spark.table(\"golds.dim_title\")\n",
    "    .select(\n",
    "        F.col(\"id_title\").cast(\"bigint\").alias(\"id_title\"),\n",
    "        norm_title(F.col(\"title\")).alias(\"k_title\")\n",
    "    )\n",
    "    .dropDuplicates([\"id_title\", \"k_title\"])\n",
    ")\n",
    "\n",
    "dim_time = (\n",
    "    spark.table(\"golds.dim_time\")\n",
    "    .select(\n",
    "        F.col(\"id_time\").cast(\"int\").alias(\"id_time\"),\n",
    "        F.col(\"year\").cast(\"int\").alias(\"year\")\n",
    "    )\n",
    "    .dropDuplicates([\"id_time\", \"year\"])\n",
    ")\n",
    "\n",
    "# mapear Year -> id_time (usa linhas ano-only / primeiro id_time do ano)\n",
    "year_to_id_time = (\n",
    "    dim_time\n",
    "    .groupBy(\"year\")\n",
    "    .agg(F.min(\"id_time\").alias(\"id_time_year\"))\n",
    ")\n",
    "\n",
    "# --------- juntar título + tempo ----------\n",
    "af_title = (\n",
    "    af.join(dim_title, \"k_title\", \"inner\")\n",
    ")\n",
    "\n",
    "af_title_time = (\n",
    "    af_title\n",
    "    .join(year_to_id_time, af_title.year == year_to_id_time.year, \"left\")\n",
    "    .select(\n",
    "        af_title.k_name,\n",
    "        af_title.id_title,\n",
    "        af_title.year,\n",
    "        F.col(\"id_time_year\").cast(\"int\").alias(\"id_time\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# --------- juntar dim_person ----------\n",
    "# actor_films traz só actores, portanto person_type esperado = 'actor' na dim_person\n",
    "part_join = (\n",
    "    af_title_time\n",
    "    .join(\n",
    "        dim_person.filter(F.col(\"person_type\") == \"actor\"),\n",
    "        on=\"k_name\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    .select(\n",
    "        F.col(\"id_person\").cast(\"int\").alias(\"id_person\"),\n",
    "        F.col(\"id_title\").cast(\"bigint\").alias(\"id_title\"),\n",
    "        F.col(\"id_time\").cast(\"int\").alias(\"id_time\"),\n",
    "        F.col(\"person_type\")\n",
    "    )\n",
    ")\n",
    "\n",
    "participation_fact = (\n",
    "    part_join\n",
    "    .dropDuplicates([\"id_person\", \"id_title\", \"id_time\"])\n",
    ")\n",
    "\n",
    "# --------- write ----------\n",
    "(\n",
    "    participation_fact\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"parquet\")\n",
    "    .option(\"path\", OUT_PATH)\n",
    "    .saveAsTable(\"golds.participation\")\n",
    ")\n",
    "\n",
    "# --------- quick checks ----------\n",
    "spark.sql(\"SELECT COUNT(*) AS n FROM golds.participation\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "  SUM(CASE WHEN id_person IS NULL THEN 1 ELSE 0 END) AS null_id_person,\n",
    "  SUM(CASE WHEN id_title  IS NULL THEN 1 ELSE 0 END) AS null_id_title,\n",
    "  SUM(CASE WHEN id_time   IS NULL THEN 1 ELSE 0 END) AS null_id_time\n",
    "FROM golds.participation\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT person_type, COUNT(*) AS n\n",
    "FROM golds.participation\n",
    "GROUP BY person_type\n",
    "ORDER BY n DESC\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "spark.sql(\"SELECT * FROM golds.participation LIMIT 50\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eccda00-d5a5-471e-a5fc-918cec9e4ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33baf8ac-99d4-49cd-863a-a1b4499a397b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
